{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Strategy Comparison: 6 Strategies x 2 Base Models\n\nTest which base model + prompt combo works best **without any fine-tuning**.\n\n**No post-processing** - only prompt changes (within competition rules).\n\nBoth models get the full 12-criteria scoring schedule in the prompt.\n\nModels: Qwen2.5-14B-Instruct (dense) vs gpt-oss-20b (MoE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Strategy Comparison Test\n\n**24 configs**: 6 prompt strategies x 4 models. No post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport gc\nimport json\nimport re\nimport glob\nimport random\nimport numpy as np\nimport torch\nfrom collections import defaultdict\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Optional, Set, Dict, Any\nfrom unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper Game Class\n\n**DO NOT MODIFY** - must match evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\nclass MinesweeperGame:\n    rows: int\n    cols: int\n    num_mines: int\n    seed: Optional[int] = None\n    _rng: random.Random = field(init=False, repr=False)\n    _board: List[List[int]] = field(init=False, repr=False)\n    _revealed: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n    _flagged: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n    _state: str = field(default=\"ongoing\", init=False, repr=False)\n\n    def __post_init__(self):\n        if self.num_mines >= self.rows * self.cols:\n            raise ValueError(\"Too many mines for board size\")\n        self._rng = random.Random(self.seed)\n        self._board = [[0 for _ in range(self.cols)] for _ in range(self.rows)]\n        self._place_mines()\n        self._calculate_numbers()\n\n    def _place_mines(self):\n        positions = [(r, c) for r in range(self.rows) for c in range(self.cols)]\n        mine_positions = self._rng.sample(positions, self.num_mines)\n        for r, c in mine_positions:\n            self._board[r][c] = -1\n\n    def _calculate_numbers(self):\n        for r in range(self.rows):\n            for c in range(self.cols):\n                if self._board[r][c] == -1:\n                    continue\n                count = 0\n                for dr in [-1, 0, 1]:\n                    for dc in [-1, 0, 1]:\n                        if dr == 0 and dc == 0:\n                            continue\n                        nr, nc = r + dr, c + dc\n                        if 0 <= nr < self.rows and 0 <= nc < self.cols:\n                            if self._board[nr][nc] == -1:\n                                count += 1\n                self._board[r][c] = count\n\n    def _reveal_cell(self, row, col):\n        if not (0 <= row < self.rows and 0 <= col < self.cols):\n            return False\n        if (row, col) in self._revealed or (row, col) in self._flagged:\n            return False\n        stack = [(row, col)]\n        while stack:\n            r, c = stack.pop()\n            if (r, c) in self._revealed:\n                continue\n            self._revealed.add((r, c))\n            if self._board[r][c] == -1:\n                self._state = \"failed\"\n                return True\n            if self._board[r][c] == 0:\n                for dr in [-1, 0, 1]:\n                    for dc in [-1, 0, 1]:\n                        if dr == 0 and dc == 0:\n                            continue\n                        nr, nc = r + dr, c + dc\n                        if (0 <= nr < self.rows and 0 <= nc < self.cols\n                                and (nr, nc) not in self._revealed\n                                and (nr, nc) not in self._flagged):\n                            stack.append((nr, nc))\n        return True\n\n    def _flag_cell(self, row, col):\n        if not (0 <= row < self.rows and 0 <= col < self.cols):\n            return False\n        if (row, col) in self._revealed:\n            return False\n        if (row, col) in self._flagged:\n            self._flagged.remove((row, col))\n        else:\n            self._flagged.add((row, col))\n        return True\n\n    def do_action(self, action):\n        if self._state != \"ongoing\":\n            return \"game_over\"\n        if not isinstance(action, dict):\n            self._state = \"failed\"\n            return \"invalid_format\"\n        action_type = action.get(\"type\")\n        row = action.get(\"row\")\n        col = action.get(\"col\")\n        if action_type not in [\"reveal\", \"flag\"] or row is None or col is None:\n            self._state = \"failed\"\n            return \"invalid_format\"\n        try:\n            row, col = int(row), int(col)\n        except (ValueError, TypeError):\n            self._state = \"failed\"\n            return \"invalid_format\"\n        if not (0 <= row < self.rows and 0 <= col < self.cols):\n            self._state = \"failed\"\n            return \"out_of_bounds\"\n        if action_type == \"reveal\":\n            if (row, col) in self._revealed:\n                self._state = \"failed\"\n                return \"already_revealed\"\n            if (row, col) in self._flagged:\n                self._state = \"failed\"\n                return \"flagged_cell\"\n            valid = self._reveal_cell(row, col)\n        else:\n            if (row, col) in self._revealed:\n                self._state = \"failed\"\n                return \"invalid_flag\"\n            valid = self._flag_cell(row, col)\n        if not valid:\n            self._state = \"failed\"\n            return \"invalid_format\"\n        self._check_win()\n        if self._state == \"failed\":\n            return \"mine\"\n        if self._state == \"success\":\n            return \"win\"\n        return \"ok\"\n\n    def _check_win(self):\n        total_cells = self.rows * self.cols\n        safe_cells = total_cells - self.num_mines\n        if len(self._revealed) == safe_cells:\n            self._state = \"success\"\n\n    def get_visible_board(self):\n        visible = []\n        for r in range(self.rows):\n            row = []\n            for c in range(self.cols):\n                if (r, c) in self._flagged:\n                    row.append('F')\n                elif (r, c) in self._revealed:\n                    val = self._board[r][c]\n                    row.append('*' if val == -1 else str(val))\n                else:\n                    row.append('.')\n            visible.append(row)\n        return visible\n\n    def state(self):\n        return self._state\n\nprint(\"MinesweeperGame class loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n\nCompact prompt builder, JSON parser, logical deduction checker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(r, c, rows, cols):\n    neighbors = []\n    for dr in [-1, 0, 1]:\n        for dc in [-1, 0, 1]:\n            if dr == 0 and dc == 0:\n                continue\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < rows and 0 <= nc < cols:\n                neighbors.append((nr, nc))\n    return neighbors\n\n\ndef build_compact_prompt(game_or_state):\n    \"\"\"Build compact prompt. Accepts MinesweeperGame or dict game state.\"\"\"\n    if isinstance(game_or_state, dict):\n        board = game_or_state[\"board\"]\n        rows = game_or_state[\"rows\"]\n        cols = game_or_state[\"cols\"]\n        mines = game_or_state[\"mines\"]\n        flagged = game_or_state.get(\"flags_placed\", 0)\n        revealed = game_or_state.get(\"cells_revealed\", 0)\n    else:\n        board = game_or_state.get_visible_board()\n        rows = game_or_state.rows\n        cols = game_or_state.cols\n        mines = game_or_state.num_mines\n        flagged = len(game_or_state._flagged)\n        revealed = len(game_or_state._revealed)\n\n    board_lines = []\n    for r in range(rows):\n        board_lines.append(f\"{r:>2}|{''.join(board[r])}\")\n    board_str = \"\\n\".join(board_lines)\n\n    prompt = f\"\"\"Minesweeper {rows}x{cols}, {mines} mines, {flagged} flagged, {revealed} revealed.\n.=unknown F=flag 0-8=adjacent mines\n\n{board_str}\n\nJSON action:\"\"\"\n    return prompt\n\n\ndef parse_llm_action(response):\n    \"\"\"Extract JSON action from LLM response. Takes LAST valid match.\"\"\"\n    best = None\n    for match in re.finditer(r'\\{[^{}]*\\}', response):\n        try:\n            action = json.loads(match.group())\n            if (\"type\" in action and \"row\" in action and \"col\" in action\n                    and action[\"type\"] in [\"reveal\", \"flag\"]):\n                best = action\n        except json.JSONDecodeError:\n            continue\n    return best\n\n\ndef is_logically_deducible(board, rows, cols, action_type, tr, tc):\n    \"\"\"Check if a move can be logically deduced from board constraints.\"\"\"\n    cf = set()\n    cr = set()\n\n    # Phase 1: Single-cell constraint propagation\n    changed = True\n    while changed:\n        changed = False\n        for r in range(rows):\n            for c in range(cols):\n                if board[r][c] not in '12345678':\n                    continue\n                num = int(board[r][c])\n                nbrs = get_neighbors(r, c, rows, cols)\n                fn = sum(1 for nr, nc in nbrs if board[nr][nc] == 'F' or (nr, nc) in cf)\n                un = [(nr, nc) for nr, nc in nbrs\n                      if board[nr][nc] == '.' and (nr, nc) not in cf and (nr, nc) not in cr]\n                rem = num - fn\n                if rem < 0:\n                    continue\n                if rem == len(un) and un:\n                    for n in un:\n                        if n not in cf:\n                            cf.add(n)\n                            changed = True\n                if rem == 0 and un:\n                    for n in un:\n                        if n not in cr:\n                            cr.add(n)\n                            changed = True\n\n    # Phase 2: Coupled constraints (pair-wise subset analysis)\n    numbered = [(r, c) for r in range(rows) for c in range(cols) if board[r][c] in '12345678']\n    changed = True\n    iters = 0\n    while changed and iters < 30:\n        changed = False\n        iters += 1\n        for i, (r1, c1) in enumerate(numbered):\n            n1 = int(board[r1][c1])\n            nb1 = get_neighbors(r1, c1, rows, cols)\n            f1 = sum(1 for nr, nc in nb1 if board[nr][nc] == 'F' or (nr, nc) in cf)\n            u1 = set(n for n in nb1 if board[n[0]][n[1]] == '.' and n not in cf and n not in cr)\n            rm1 = n1 - f1\n            if not u1:\n                continue\n            for j in range(i + 1, len(numbered)):\n                r2, c2 = numbered[j]\n                if abs(r1 - r2) > 2 or abs(c1 - c2) > 2:\n                    continue\n                n2 = int(board[r2][c2])\n                nb2 = get_neighbors(r2, c2, rows, cols)\n                f2 = sum(1 for nr, nc in nb2 if board[nr][nc] == 'F' or (nr, nc) in cf)\n                u2 = set(n for n in nb2 if board[n[0]][n[1]] == '.' and n not in cf and n not in cr)\n                rm2 = n2 - f2\n                if not u2:\n                    continue\n                for sa, sb, ra, rb in [(u1, u2, rm1, rm2), (u2, u1, rm2, rm1)]:\n                    if sa.issubset(sb):\n                        diff = sb - sa\n                        dm = rb - ra\n                        if diff and dm == len(diff):\n                            for cell in diff:\n                                if cell not in cf:\n                                    cf.add(cell)\n                                    changed = True\n                        elif diff and dm == 0:\n                            for cell in diff:\n                                if cell not in cr:\n                                    cr.add(cell)\n                                    changed = True\n\n    target = (tr, tc)\n    return (action_type == \"flag\" and target in cf) or (action_type == \"reveal\" and target in cr)\n\nprint(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Prompt Strategies + Scoring Rules\n\n| # | Strategy | Key Idea |\n|---|----------|----------|\n| V1 | Simple | \"You output JSON actions\" |\n| V2 | Constraint | Count flags/unknowns logic |\n| V3 | Aggressive | \"DO NOT DO NOT DO NOT\" x10, TRIPLE CHECK |\n| V4 | Rules list | STEP 1-5 verification process |\n| V5 | Annotated | Valid target cells listed: `(2,3) (2,4)...` |\n| V6 | CoT verify | Self-check: \"Cell shows: . Is it dot? YES\" |\n\nFor BASE model only: full 12-criteria scoring schedule added to every prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCORING_RULES = (\n    \"\\nSCORING RULES (maximize your total score!):\\n\"\n    \"GOOD MOVES:\\n\"\n    \"- Reveal a safe unknown(.) cell: +10 pts (or +15 if logically deducible from constraints!)\\n\"\n    \"- Flag a cell that IS a mine: +15 pts\\n\"\n    \"- Win the game (all safe cells revealed): +100 pts BONUS\\n\"\n    \"BAD MOVES (AVOID THESE AT ALL COSTS):\\n\"\n    \"- Reveal a mine: -25 pts AND GAME OVER\\n\"\n    \"- Flag a cell that is NOT a mine: -10 pts\\n\"\n    \"- Reveal an already-revealed cell (showing 0-8): -12 pts\\n\"\n    \"- Reveal a flagged cell (F): -8 pts\\n\"\n    \"- Flag an already-flagged cell (F): -8 pts\\n\"\n    \"- Flag a revealed cell (0-8): -8 pts\\n\"\n    \"- Out of bounds row/col: -15 pts\\n\"\n    \"- Placing more flags than total mines: -10 pts\\n\"\n    \"- Invalid/missing JSON: -10 pts\\n\"\n    \"STRATEGY: ONLY pick cells marked '.' (dot). Use constraint logic for +15 bonus. Never guess blindly.\"\n)\n\n\ndef strategy_v1_simple(game, add_scoring=False):\n    \"\"\"V1: Original simple prompt\"\"\"\n    prompt = build_compact_prompt(game)\n    sys = \"You output JSON actions for Minesweeper. No text, only JSON.\"\n    if add_scoring:\n        sys += SCORING_RULES\n    return sys, prompt\n\n\ndef strategy_v2_constraint(game, add_scoring=False):\n    \"\"\"V2: Constraint logic prompt\"\"\"\n    prompt = build_compact_prompt(game)\n    sys = 'Analyze the Minesweeper board. For each numbered cell, count adjacent flags(F) and unknowns(.). If number equals flag count, unknowns are safe to reveal. If number minus flags equals unknown count, unknowns are mines to flag. Only act on certain deductions. Output ONLY JSON: {\"type\":\"reveal\",\"row\":N,\"col\":N} or {\"type\":\"flag\",\"row\":N,\"col\":N}'\n    if add_scoring:\n        sys += SCORING_RULES\n    return sys, prompt\n\n\ndef strategy_v3_aggressive(game, add_scoring=False):\n    \"\"\"V3: Extremely aggressive DO NOT repeated warnings\"\"\"\n    prompt = build_compact_prompt(game)\n    sys = (\n        'You play Minesweeper. Output ONLY valid JSON: {\"type\":\"reveal\"or\"flag\",\"row\":N,\"col\":N}\\n'\n        'ABSOLUTE RULES - VIOLATION = INSTANT PENALTY:\\n'\n        '1. ONLY target cells showing \".\" on the board. These are UNKNOWN cells.\\n'\n        '2. Cells showing 0,1,2,3,4,5,6,7,8 are ALREADY REVEALED. DO NOT pick them. DO NOT DO NOT DO NOT.\\n'\n        '3. Cells showing F are ALREADY FLAGGED. DO NOT reveal them. DO NOT flag them again. DO NOT DO NOT.\\n'\n        '4. NEVER reveal an already-revealed cell. CHECK the board at your target (row,col). Is it \".\"? If NOT, STOP and pick another.\\n'\n        '5. NEVER reveal a flagged cell (F). CHECK AGAIN. Is your target \".\"? If NOT, pick another cell.\\n'\n        '6. NEVER flag an already-flagged cell. CHECK AGAIN.\\n'\n        '7. NEVER flag a revealed cell (showing 0-8). CHECK AGAIN.\\n'\n        '8. BEFORE outputting: look at board[row][col]. If it is NOT \".\", you MUST change your answer.\\n'\n        '9. DOUBLE CHECK: Is your target cell \".\"? YES -> output. NO -> pick a different cell.\\n'\n        '10. TRIPLE CHECK: Are you absolutely sure the cell shows \".\"? Confirmed? Then output.\\n'\n        'LOGIC: Count F and \".\" around numbered cells. number=flags -> unknowns safe. number-flags=unknowns -> unknowns are mines.'\n    )\n    if add_scoring:\n        sys += SCORING_RULES\n    return sys, prompt\n\n\ndef strategy_v4_rules_list(game, add_scoring=False):\n    \"\"\"V4: Step-by-step verification rules\"\"\"\n    prompt = build_compact_prompt(game)\n    sys = (\n        'Minesweeper action rules:\\n'\n        'STEP 1: Scan the board. Cells with \".\" are UNKNOWN (valid targets). Cells with 0-8 are REVEALED (forbidden). F = FLAGGED (forbidden).\\n'\n        'STEP 2: For each numbered cell (1-8), count adjacent F (flags) and \".\" (unknowns).\\n'\n        'STEP 3: If number = flag_count, all adjacent \".\" are safe -> reveal one.\\n'\n        'STEP 4: If number - flag_count = unknown_count, all adjacent \".\" are mines -> flag one.\\n'\n        'STEP 5: Pick your target (row, col). VERIFY: What does board[row][col] show?\\n'\n        '  - Shows \".\" -> GOOD, proceed.\\n'\n        '  - Shows 0-8 -> FORBIDDEN! Already revealed. Pick a different cell.\\n'\n        '  - Shows F -> FORBIDDEN! Already flagged. Pick a different cell.\\n'\n        'Output ONLY: {\"type\":\"reveal\",\"row\":N,\"col\":N} or {\"type\":\"flag\",\"row\":N,\"col\":N}'\n    )\n    if add_scoring:\n        sys += SCORING_RULES\n    return sys, prompt\n\n\ndef strategy_v5_annotated_board(game, add_scoring=False):\n    \"\"\"V5: List valid target cells directly in user prompt\"\"\"\n    board = game.get_visible_board()\n    rows, cols = game.rows, game.cols\n    flagged = len(game._flagged)\n    revealed = len(game._revealed)\n    mines = game.num_mines\n\n    board_lines = []\n    for r in range(rows):\n        board_lines.append(f\"{r:>2}|{''.join(board[r])}\")\n    board_str = \"\\n\".join(board_lines)\n\n    valid_targets = []\n    for r in range(rows):\n        for c in range(cols):\n            if board[r][c] == '.':\n                valid_targets.append(f\"({r},{c})\")\n    valid_str = \" \".join(valid_targets[:30])\n\n    prompt = (\n        f\"Minesweeper {rows}x{cols}, {mines} mines, {flagged} flagged, {revealed} revealed.\\n\"\n        f\".=unknown F=flag 0-8=adjacent mines\\n\\n\"\n        f\"{board_str}\\n\\n\"\n        f\"VALID TARGETS (only these cells show '.'): {valid_str}\\n\"\n        f\"You MUST pick row,col from this list. Any cell NOT showing '.' = HEAVY PENALTY.\\n\\n\"\n        f\"JSON action:\"\n    )\n    sys = 'Analyze the Minesweeper board. Pick ONLY from the VALID TARGETS list shown in the prompt. Output JSON: {\"type\":\"reveal\"or\"flag\",\"row\":N,\"col\":N}'\n    if add_scoring:\n        sys += SCORING_RULES\n    return sys, prompt\n\n\ndef strategy_v6_cot_verify(game, add_scoring=False):\n    \"\"\"V6: Model self-verifies target cell in think field\"\"\"\n    prompt = build_compact_prompt(game)\n    sys = (\n        'Play Minesweeper. Output JSON with self-verification:\\n'\n        '{\"think\":\"<reasoning>. Cell at (row,col) shows: <symbol>. Is it dot? YES.\",\"type\":\"reveal\"or\"flag\",\"row\":N,\"col\":N}\\n'\n        'MANDATORY: In your \"think\" field, CHECK what your target cell shows on the board.\\n'\n        'If board[row][col] is NOT \".\" (dot), you picked wrong. Numbers 0-8 = already revealed. F = already flagged.\\n'\n        'Only \".\" cells are valid targets. VERIFY before output.\\n'\n        'Logic: count F and \".\" neighbors of each number. number=F_count -> \".\" safe. number-F_count=unknown_count -> \".\" mines.'\n    )\n    if add_scoring:\n        sys += SCORING_RULES\n    return sys, prompt\n\n\nstrategies = [\n    (\"V1-simple\", strategy_v1_simple),\n    (\"V2-constraint\", strategy_v2_constraint),\n    (\"V3-aggressive\", strategy_v3_aggressive),\n    (\"V4-rules\", strategy_v4_rules_list),\n    (\"V5-annotated\", strategy_v5_annotated_board),\n    (\"V6-cot-verify\", strategy_v6_cot_verify),\n]\n\nprint(f\"Defined {len(strategies)} prompt strategies.\")\nprint(\"Scoring rules will be added to BASE model prompts only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run All 12 Tests (2 base models x 6 strategies)\n\n**2 Base Models (no fine-tuning):**\n- `BASE-QWEN-14B`: Qwen2.5-14B-Instruct (14B dense, instruction-tuned)\n- `BASE-OSS-20B`: gpt-oss-20b (20B MoE, 3.6B active, NOT instruction-tuned)\n\nBoth get the full **12-criteria scoring schedule** injected into every prompt.\n\n**4 Board configs**: 8x8(x3), 10x10(x3), 6x10(x3), 16x16(x2) = 11 games per config\n\n**Total: 12 configs x 11 games = 132 games**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_with_strategy(model, tokenizer, strategy_fn, add_scoring=False,\n                            rows=8, cols=8, num_mines=10, seed=None, max_moves=200):\n    \"\"\"Play one full game with a given prompt strategy. No post-processing.\"\"\"\n    game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n    game.do_action({\"type\": \"reveal\", \"row\": rows // 2, \"col\": cols // 2})\n    moves = 0\n    score = 0.0\n    consecutive_bad = 0\n    invalid_counts = {\"already_revealed\": 0, \"reveal_flagged\": 0, \"already_flagged\": 0,\n                      \"flag_revealed\": 0, \"oob\": 0, \"mine_hit\": 0, \"wrong_flag\": 0, \"invalid_json\": 0}\n\n    while game.state() == \"ongoing\" and moves < max_moves and consecutive_bad < 5:\n        sys_prompt, user_prompt = strategy_fn(game, add_scoring=add_scoring)\n        msgs = [{\"role\": \"system\", \"content\": sys_prompt},\n                {\"role\": \"user\", \"content\": user_prompt}]\n        try:\n            text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                add_generation_prompt=True, enable_thinking=False)\n        except TypeError:\n            text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                add_generation_prompt=True)\n        inp = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        out = model.generate(**inp, temperature=0.3, max_new_tokens=128, do_sample=True)\n        resp = tokenizer.decode(out[0][inp.input_ids.shape[1]:], skip_special_tokens=True)\n        action = parse_llm_action(resp)\n        moves += 1\n\n        if action is None:\n            score -= 10.0; consecutive_bad += 1; invalid_counts[\"invalid_json\"] += 1; continue\n        try:\n            row, col = int(action[\"row\"]), int(action[\"col\"])\n        except (ValueError, TypeError):\n            score -= 10.0; consecutive_bad += 1; invalid_counts[\"invalid_json\"] += 1; continue\n        atype = action[\"type\"]\n\n        if not (0 <= row < rows and 0 <= col < cols):\n            score -= 15.0; consecutive_bad += 1; invalid_counts[\"oob\"] += 1; continue\n        if atype == \"reveal\":\n            if (row, col) in game._revealed:\n                score -= 12.0; consecutive_bad += 1; invalid_counts[\"already_revealed\"] += 1; continue\n            if (row, col) in game._flagged:\n                score -= 8.0; consecutive_bad += 1; invalid_counts[\"reveal_flagged\"] += 1; continue\n            if game._board[row][col] == -1:\n                score -= 25.0; invalid_counts[\"mine_hit\"] += 1; break\n            consecutive_bad = 0\n            board = game.get_visible_board()\n            is_log = is_logically_deducible(board, rows, cols, \"reveal\", row, col)\n            score += 15.0 if is_log else 10.0\n            game.do_action(action)\n            if game.state() == \"success\":\n                score += 100.0\n        elif atype == \"flag\":\n            if (row, col) in game._revealed:\n                score -= 8.0; consecutive_bad += 1; invalid_counts[\"flag_revealed\"] += 1; continue\n            if (row, col) in game._flagged:\n                score -= 8.0; consecutive_bad += 1; invalid_counts[\"already_flagged\"] += 1; continue\n            consecutive_bad = 0\n            if len(game._flagged) + 1 > num_mines:\n                score -= 10.0\n            if game._board[row][col] == -1:\n                score += 15.0\n            else:\n                score -= 10.0; invalid_counts[\"wrong_flag\"] += 1\n            game.do_action(action)\n\n    return {\n        \"result\": game.state(),\n        \"moves\": moves,\n        \"score\": score,\n        \"invalid\": invalid_counts,\n    }\n\n\n# ---- Board configs: 3 normal + 1 bigger ----\ntest_configs = [\n    (8, 8, 10, 3, \"8x8\"),\n    (10, 10, 15, 3, \"10x10\"),\n    (6, 10, 8, 3, \"6x10\"),\n    (16, 16, 40, 2, \"16x16\"),     # bigger board\n]\ntotal_games_per_strat = sum(n for _, _, _, n, _ in test_configs)\nprint(f\"Test configs: {[lbl for *_, lbl in test_configs]}\")\nprint(f\"Games per strategy: {total_games_per_strat}\")\n\n# ---- Model configs: BASE models only (no fine-tuning) ----\n# Auto-detect base model paths from cache\n\ndef _find_model_path(search_dirs):\n    \"\"\"Find the snapshot path for a model in the HF cache.\"\"\"\n    for md in search_dirs:\n        if os.path.exists(md):\n            snaps = sorted(glob.glob(os.path.join(md, \"snapshots\", \"*\")))\n            if snaps:\n                return snaps[-1]\n    return None\n\n_qwen_base = _find_model_path([\n    \"/root/.cache/huggingface/models--Qwen--Qwen2.5-14B-Instruct\",\n])\nif _qwen_base is None:\n    _qwen_base = \"Qwen/Qwen2.5-14B-Instruct\"  # fallback\n\n_oss_base = _find_model_path([\n    \"/root/.cache/huggingface/models--unsloth--gpt-oss-20b-BF16\",\n    \"/root/.cache/huggingface/models--gpt-oss-20b\",\n])\nif _oss_base is None:\n    _oss_base = \"unsloth/gpt-oss-20b-BF16\"  # fallback\n\n# (label, path, add_scoring_to_prompt)\n# Both are base models -> scoring rules added to all prompts\nmodel_configs = [\n    (\"BASE-QWEN-14B\", _qwen_base, True),\n    (\"BASE-OSS-20B\", _oss_base, True),\n]\n\nprint(f\"\\nModels to test (BASE only, no fine-tuning):\")\nfor label, path, _ in model_configs:\n    print(f\"  {label}: {path}\")\nprint(f\"Total configs: {len(model_configs)} models x {len(strategies)} strategies = {len(model_configs) * len(strategies)}\")\nprint(f\"Total games: {len(model_configs) * len(strategies) * total_games_per_strat}\")\n\n# ---- RUN ALL TESTS ----\ngrand_results = {}\n\nfor model_label, model_path, is_base in model_configs:\n    print(\"\\n\" + \"#\" * 80)\n    print(f\"#  LOADING: {model_label}\")\n    print(f\"#  Path: {model_path}\")\n    print(\"#\" * 80)\n\n    # Check if model exists\n    if not os.path.exists(model_path) and not model_path.startswith(\"/root\"):\n        print(f\"  WARNING: {model_path} not found, SKIPPING this model.\")\n        for strat_name, _ in strategies:\n            grand_results[f\"{model_label}|{strat_name}\"] = {\n                \"model\": model_label, \"strategy\": strat_name,\n                \"wins\": 0, \"total\": 0, \"avg_score\": 0, \"invalid\": {},\n                \"skipped\": True,\n            }\n        continue\n\n    # Cleanup\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    try:\n        _model, _tokenizer = FastLanguageModel.from_pretrained(\n            model_name=model_path,\n            load_in_4bit=False,\n            max_seq_length=4096,\n            torch_dtype=torch.bfloat16,\n        )\n        FastLanguageModel.for_inference(_model)\n        print(f\"  Loaded on {_model.device}\")\n    except Exception as e:\n        print(f\"  FAILED to load: {e}\")\n        for strat_name, _ in strategies:\n            grand_results[f\"{model_label}|{strat_name}\"] = {\n                \"model\": model_label, \"strategy\": strat_name,\n                \"wins\": 0, \"total\": 0, \"avg_score\": 0, \"invalid\": {},\n                \"skipped\": True,\n            }\n        continue\n\n    for strat_name, strat_fn in strategies:\n        config_key = f\"{model_label}|{strat_name}\"\n        print(f\"\\n{'='*60}\")\n        print(f\"  {config_key}\")\n        print(f\"{'='*60}\")\n\n        strat_results = []\n        total_invalid = {\"already_revealed\": 0, \"reveal_flagged\": 0, \"already_flagged\": 0,\n                         \"flag_revealed\": 0, \"oob\": 0, \"mine_hit\": 0, \"wrong_flag\": 0, \"invalid_json\": 0}\n\n        for rows, cols, mines, num_seeds, label in test_configs:\n            wins = 0\n            total_score = 0.0\n            total_moves = 0\n            for seed_i in range(num_seeds):\n                res = play_game_with_strategy(\n                    _model, _tokenizer, strat_fn,\n                    add_scoring=is_base,\n                    rows=rows, cols=cols, num_mines=mines, seed=42 + seed_i\n                )\n                strat_results.append(res)\n                total_score += res[\"score\"]\n                total_moves += res[\"moves\"]\n                if res[\"result\"] == \"success\":\n                    wins += 1\n                for k, v in res[\"invalid\"].items():\n                    total_invalid[k] += v\n            avg_sc = total_score / num_seeds\n            print(f\"    {label}: {wins}/{num_seeds} wins, avg {total_moves/num_seeds:.1f} moves, avg score {avg_sc:+.1f}\")\n\n        total_games = len(strat_results)\n        total_wins = sum(1 for r in strat_results if r[\"result\"] == \"success\")\n        total_avg = sum(r[\"score\"] for r in strat_results) / total_games if total_games else 0\n        print(f\"\\n    TOTAL: {total_wins}/{total_games} wins, avg score {total_avg:+.1f}\")\n        inv_str = \", \".join(f\"{k}={v}\" for k, v in total_invalid.items() if v > 0)\n        print(f\"    Invalid: {inv_str if inv_str else 'NONE'}\")\n\n        grand_results[config_key] = {\n            \"model\": model_label, \"strategy\": strat_name,\n            \"wins\": total_wins, \"total\": total_games,\n            \"avg_score\": total_avg, \"invalid\": total_invalid,\n            \"skipped\": False,\n        }\n\n    # Cleanup model\n    del _model\n    del _tokenizer\n    gc.collect()\n    torch.cuda.empty_cache()\n    print(f\"\\n  Unloaded {model_label}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: Grand Ranking\n\nSorted by average score. Shows best strategy per model and overall winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out skipped\nactive = {k: v for k, v in grand_results.items() if not v.get(\"skipped\")}\n\nprint(\"\\n\" + \"=\" * 95)\nprint(\"GRAND RANKING: ALL CONFIGURATIONS (sorted by average score)\")\nprint(\"=\" * 95)\nprint(f\"{'#':>3} {'Model':>16} | {'Strategy':>15} | {'Avg Score':>10} | {'Wins':>8} | {'Invalids':>8} | Top Penalties\")\nprint(\"-\" * 95)\nranked = sorted(active.items(), key=lambda x: x[1][\"avg_score\"], reverse=True)\nfor rank, (key, data) in enumerate(ranked, 1):\n    inv_total = sum(data[\"invalid\"].values())\n    top_penalties = sorted(data[\"invalid\"].items(), key=lambda x: x[1], reverse=True)\n    top_str = \", \".join(f\"{k}={v}\" for k, v in top_penalties[:3] if v > 0)\n    print(f\"{rank:>3} {data['model']:>16} | {data['strategy']:>15} | {data['avg_score']:>+10.1f} | \"\n          f\"{data['wins']:>3}/{data['total']:<3} | {inv_total:>8} | {top_str}\")\nprint(\"=\" * 95)\n\n# Best per model\nprint(\"\\nBEST STRATEGY PER MODEL:\")\nfor ml in [\"BASE-QWEN-14B\", \"BASE-OSS-20B\"]:\n    model_results = [(k, v) for k, v in active.items() if v[\"model\"] == ml]\n    if model_results:\n        best_key, best_data = max(model_results, key=lambda x: x[1][\"avg_score\"])\n        inv_total = sum(best_data[\"invalid\"].values())\n        print(f\"  {ml:>16}: {best_data['strategy']:>15} | avg {best_data['avg_score']:+.1f} | \"\n              f\"{best_data['wins']}/{best_data['total']} wins | {inv_total} invalids\")\n\n# Overall best\nif ranked:\n    best_key, best_data = ranked[0]\n    print(f\"\\n{'='*60}\")\n    print(f\">>> OVERALL BEST: {best_key}\")\n    print(f\">>>   avg score: {best_data['avg_score']:+.1f}\")\n    print(f\">>>   wins: {best_data['wins']}/{best_data['total']}\")\n    print(f\">>>   invalids: {sum(best_data['invalid'].values())}\")\n    print(f\"{'='*60}\")\n    print(\"\\nGive these results to Claude to write the winning combo to agents/!\")\n\n# Skipped models\nskipped = {k: v for k, v in grand_results.items() if v.get(\"skipped\")}\nif skipped:\n    print(f\"\\nSKIPPED (model not found): {set(v['model'] for v in skipped.values())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}