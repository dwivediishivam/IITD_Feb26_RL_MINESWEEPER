{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper LLM - gpt-oss-20b Pipeline\n\n## Model: gpt-oss-20b (MoE, 3.6B active params)\n\n- **Not instruction-tuned** \u2192 2 SFT epochs\n- **BF16** precision, no 4-bit quantization\n- **LoRA r=64** for more capacity\n- All Phase 2 improvements baked in from the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\nimport glob\nimport shutil\n\n# Point HuggingFace to the pre-downloaded model cache\nos.environ[\"HF_HOME\"] = \"/root/.cache/huggingface\"\nos.environ[\"HF_HUB_CACHE\"] = \"/root/.cache/huggingface\"\nos.environ[\"TRANSFORMERS_CACHE\"] = \"/root/.cache/huggingface\"\n\n# Discover available models and their snapshot paths\nprint(\"=\" * 60)\nprint(\"AVAILABLE MODELS IN CACHE:\")\nprint(\"=\" * 60)\ncache_dir = \"/root/.cache/huggingface\"\nmodel_dirs = sorted(glob.glob(os.path.join(cache_dir, \"models--*\")))\nif model_dirs:\n    for d in model_dirs:\n        name = os.path.basename(d).replace(\"models--\", \"\").replace(\"--\", \"/\")\n        snapshots = sorted(glob.glob(os.path.join(d, \"snapshots\", \"*\")))\n        print(f\"  {name}\")\n        for s in snapshots:\n            print(f\"    snapshot: {s}\")\nelse:\n    print(\"  No models found in cache - will try HF download\")\n\nimport json\nimport random\nimport re\nimport time\nimport numpy as np\nfrom dataclasses import dataclass, field\nfrom typing import List, Tuple, Optional, Set, Dict\nfrom collections import defaultdict\nfrom datasets import Dataset\n\nprint(\"\\nAll imports ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gpt-oss-20b Model\n\n**gpt-oss-20b**: MoE model (20B total, 3.6B active params)\n- NOT instruction-tuned \u2192 needs more SFT training (2 epochs)\n- BF16 precision (no 4-bit quantization for MoE)\n- LoRA rank 64 for more capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-oss-20b: MoE model with 20B total params, 3.6B active\n# NOT instruction-tuned - needs more SFT training\n# Use BF16 (no 4-bit quantization for MoE models)\n# ################################################################\nfrom unsloth import FastLanguageModel\nimport torch\n\nmax_seq_length = 4096\nlora_rank = 64  # Higher rank for non-instruct model\n\n# Force gpt-oss-20b\nmodel_dir = \"/root/.cache/huggingface/models--unsloth--gpt-oss-20b-BF16\"\nmodel_name = None\nif os.path.exists(model_dir):\n    snapshots = sorted(glob.glob(os.path.join(model_dir, \"snapshots\", \"*\")))\n    if snapshots:\n        model_name = snapshots[-1]\n        print(f\"Found gpt-oss-20b: {model_name}\")\nif model_name is None:\n    model_name = \"unsloth/gpt-oss-20b-BF16\"\n    print(f\"Will download: {model_name}\")\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=model_name,\n    load_in_4bit=False,\n    max_seq_length=max_seq_length,\n    torch_dtype=torch.bfloat16,\n)\n\n# Ensure tokenizer has chat template (gpt-oss may not have one)\nif not hasattr(tokenizer, 'chat_template') or tokenizer.chat_template is None:\n    tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'system' %}<|system|>\\n{{ message['content'] }}\\n{% elif message['role'] == 'user' %}<|user|>\\n{{ message['content'] }}\\n{% elif message['role'] == 'assistant' %}<|assistant|>\\n{{ message['content'] }}\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|assistant|>\\n{% endif %}\"\n    print(\"Added chat template for non-instruct model\")\n\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n\nprint(f\"Model loaded: {model_name}\")\nprint(f\"Device: {model.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add LoRA Adapters (r=64)\n\nHigher rank LoRA for non-instruct model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n    model,\n    r=lora_rank,\n    target_modules=[\n        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n        \"gate_proj\", \"up_proj\", \"down_proj\",\n    ],\n    lora_alpha=lora_rank * 2,\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n)\nprint(f\"LoRA adapters added (rank={lora_rank}, alpha={lora_rank * 2})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minesweeper Game Implementation\n\n**DO NOT MODIFY** - must match evaluation environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\nfrom typing import List, Tuple, Optional, Set\nimport random\n\n@dataclass\nclass MinesweeperGame:\n    rows: int\n    cols: int\n    num_mines: int\n    seed: Optional[int] = None\n    _rng: random.Random = field(init=False, repr=False)\n    _board: List[List[int]] = field(init=False, repr=False)\n    _revealed: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n    _flagged: Set[Tuple[int, int]] = field(init=False, repr=False, default_factory=set)\n    _state: str = field(default=\"ongoing\", init=False, repr=False)\n\n    def __post_init__(self):\n        if self.num_mines >= self.rows * self.cols:\n            raise ValueError(\"Too many mines for board size\")\n        self._rng = random.Random(self.seed)\n        self._board = [[0 for _ in range(self.cols)] for _ in range(self.rows)]\n        self._place_mines()\n        self._calculate_numbers()\n\n    def _place_mines(self):\n        positions = [(r, c) for r in range(self.rows) for c in range(self.cols)]\n        mine_positions = self._rng.sample(positions, self.num_mines)\n        for r, c in mine_positions:\n            self._board[r][c] = -1\n\n    def _calculate_numbers(self):\n        for r in range(self.rows):\n            for c in range(self.cols):\n                if self._board[r][c] == -1:\n                    continue\n                count = 0\n                for dr in [-1, 0, 1]:\n                    for dc in [-1, 0, 1]:\n                        if dr == 0 and dc == 0:\n                            continue\n                        nr, nc = r + dr, c + dc\n                        if 0 <= nr < self.rows and 0 <= nc < self.cols:\n                            if self._board[nr][nc] == -1:\n                                count += 1\n                self._board[r][c] = count\n\n    def _reveal_cell(self, row: int, col: int) -> bool:\n        if not (0 <= row < self.rows and 0 <= col < self.cols):\n            return False\n        if (row, col) in self._revealed or (row, col) in self._flagged:\n            return False\n        stack = [(row, col)]\n        while stack:\n            r, c = stack.pop()\n            if (r, c) in self._revealed:\n                continue\n            self._revealed.add((r, c))\n            if self._board[r][c] == -1:\n                self._state = \"failed\"\n                return True\n            if self._board[r][c] == 0:\n                for dr in [-1, 0, 1]:\n                    for dc in [-1, 0, 1]:\n                        if dr == 0 and dc == 0:\n                            continue\n                        nr, nc = r + dr, c + dc\n                        if (0 <= nr < self.rows and 0 <= nc < self.cols\n                                and (nr, nc) not in self._revealed\n                                and (nr, nc) not in self._flagged):\n                            stack.append((nr, nc))\n        return True\n\n    def _flag_cell(self, row: int, col: int) -> bool:\n        if not (0 <= row < self.rows and 0 <= col < self.cols):\n            return False\n        if (row, col) in self._revealed:\n            return False\n        if (row, col) in self._flagged:\n            self._flagged.remove((row, col))\n        else:\n            self._flagged.add((row, col))\n        return True\n\n    def do_action(self, action: dict) -> str:\n        if self._state != \"ongoing\":\n            return \"game_over\"\n        if not isinstance(action, dict):\n            self._state = \"failed\"\n            return \"invalid_format\"\n        action_type = action.get(\"type\")\n        row = action.get(\"row\")\n        col = action.get(\"col\")\n        if action_type not in [\"reveal\", \"flag\"] or row is None or col is None:\n            self._state = \"failed\"\n            return \"invalid_format\"\n        try:\n            row, col = int(row), int(col)\n        except (ValueError, TypeError):\n            self._state = \"failed\"\n            return \"invalid_format\"\n        if not (0 <= row < self.rows and 0 <= col < self.cols):\n            self._state = \"failed\"\n            return \"out_of_bounds\"\n        if action_type == \"reveal\":\n            if (row, col) in self._revealed:\n                self._state = \"failed\"\n                return \"already_revealed\"\n            if (row, col) in self._flagged:\n                self._state = \"failed\"\n                return \"flagged_cell\"\n            valid = self._reveal_cell(row, col)\n        else:\n            if (row, col) in self._revealed:\n                self._state = \"failed\"\n                return \"invalid_flag\"\n            valid = self._flag_cell(row, col)\n        if not valid:\n            self._state = \"failed\"\n            return \"invalid_format\"\n        self._check_win()\n        if self._state == \"failed\":\n            return \"mine\"\n        if self._state == \"success\":\n            return \"win\"\n        return \"ok\"\n\n    def _check_win(self):\n        total_cells = self.rows * self.cols\n        safe_cells = total_cells - self.num_mines\n        if len(self._revealed) == safe_cells:\n            self._state = \"success\"\n\n    def get_visible_board(self) -> List[List[str]]:\n        visible = []\n        for r in range(self.rows):\n            row = []\n            for c in range(self.cols):\n                if (r, c) in self._flagged:\n                    row.append('F')\n                elif (r, c) in self._revealed:\n                    val = self._board[r][c]\n                    row.append('*' if val == -1 else str(val))\n                else:\n                    row.append('.')\n            visible.append(row)\n        return visible\n\n    def state(self) -> str:\n        return self._state\n\n    def pretty_print(self) -> str:\n        visible = self.get_visible_board()\n        lines = []\n        header = \"   \" + \" \".join(f\"{i:2d}\" for i in range(self.cols))\n        lines.append(header)\n        lines.append(\"  \" + \"\\u2500\" * (self.cols * 3 + 1))\n        for r, row in enumerate(visible):\n            line = f\"{r:2d}\\u2502 \" + \"  \".join(row)\n            lines.append(line)\n        return \"\\n\".join(lines)\n\n# Quick test\ngame = MinesweeperGame(rows=6, cols=6, num_mines=5, seed=42)\ngame.do_action({\"type\": \"reveal\", \"row\": 3, \"col\": 3})\nprint(game.pretty_print())\nprint(f\"State: {game.state()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expert Solver + Compact Prompt + JSON Parser\n\nSystem prompt includes constraint logic instructions for better reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solver: Constraint propagation + coupled subset analysis\n# Tested: 56-80% win rates across board sizes\n# Compact prompt: 90% token savings vs JSON (essential for 50x50)\n# Logical deduction: Detects +15 vs +10 eval bonus\n# ################################################################\n\nSYSTEM_PROMPT = 'Analyze the Minesweeper board. CRITICAL: You can ONLY target cells marked \".\" (unknown). NEVER pick a numbered cell (0-8) or flagged cell (F) - those are already revealed/flagged. For each numbered cell, count adjacent flags(F) and unknowns(.). If number equals flag count, unknowns are safe to reveal. If number minus flags equals unknown count, unknowns are mines to flag. Only act on certain deductions. Output JSON: {\"think\":\"<brief constraint reasoning>\",\"type\":\"reveal\"or\"flag\",\"row\":N,\"col\":N}'\n\ndef build_compact_prompt(game_or_state):\n    \"\"\"Build compact prompt. Accepts MinesweeperGame or dict game state.\"\"\"\n    if isinstance(game_or_state, dict):\n        board = game_or_state[\"board\"]\n        rows = game_or_state[\"rows\"]\n        cols = game_or_state[\"cols\"]\n        mines = game_or_state[\"mines\"]\n        flagged = game_or_state.get(\"flags_placed\", 0)\n        revealed = game_or_state.get(\"cells_revealed\", 0)\n    else:\n        board = game_or_state.get_visible_board()\n        rows = game_or_state.rows\n        cols = game_or_state.cols\n        mines = game_or_state.num_mines\n        flagged = len(game_or_state._flagged)\n        revealed = len(game_or_state._revealed)\n\n    board_lines = []\n    for r in range(rows):\n        board_lines.append(f\"{r:>2}|{''.join(board[r])}\")\n    board_str = \"\\n\".join(board_lines)\n\n    prompt = f\"\"\"Minesweeper {rows}x{cols}, {mines} mines, {flagged} flagged, {revealed} revealed.\n.=unknown F=flag 0-8=adjacent mines\n\n{board_str}\n\nJSON action:\"\"\"\n    return prompt\n\n\ndef parse_llm_action(response):\n    \"\"\"Extract JSON action from LLM response. Takes LAST valid match.\"\"\"\n    best = None\n    for match in re.finditer(r'\\{[^{}]*\\}', response):\n        try:\n            action = json.loads(match.group())\n            if (\"type\" in action and \"row\" in action and \"col\" in action\n                    and action[\"type\"] in [\"reveal\", \"flag\"]):\n                best = action\n        except json.JSONDecodeError:\n            continue\n    return best\n\n\ndef get_neighbors(r, c, rows, cols):\n    neighbors = []\n    for dr in [-1, 0, 1]:\n        for dc in [-1, 0, 1]:\n            if dr == 0 and dc == 0:\n                continue\n            nr, nc = r + dr, c + dc\n            if 0 <= nr < rows and 0 <= nc < cols:\n                neighbors.append((nr, nc))\n    return neighbors\n\n\ndef generate_reasoning(board, rows, cols, action):\n    \"\"\"Generate brief chain-of-thought reasoning for a solver action.\n    Finds the numbered cell that directly constrains the target.\"\"\"\n    row, col = action[\"row\"], action[\"col\"]\n    atype = action[\"type\"]\n    # Find a numbered neighbor that directly constrains this cell\n    for r in range(rows):\n        for c in range(cols):\n            if board[r][c] not in '12345678':\n                continue\n            num = int(board[r][c])\n            nbrs = get_neighbors(r, c, rows, cols)\n            if (row, col) not in nbrs:\n                continue\n            flags = sum(1 for nr, nc in nbrs if board[nr][nc] == 'F')\n            unknowns = [(nr, nc) for nr, nc in nbrs if board[nr][nc] == '.']\n            rem = num - flags\n            if atype == \"reveal\" and (row, col) in unknowns and rem == 0:\n                return f\"({r},{c})={num}, {flags} flags, 0 mines left -> ({row},{col}) safe\"\n            if atype == \"flag\" and (row, col) in unknowns and rem == len(unknowns):\n                return f\"({r},{c})={num}, {flags}F, {len(unknowns)}U={rem} mines -> ({row},{col}) mine\"\n    # Phase 2 / coupled deduction - generic trace\n    if atype == \"flag\":\n        return f\"Constraints -> ({row},{col}) must be mine\"\n    return f\"Constraints -> ({row},{col}) is safe\"\n\n\ndef is_logically_deducible(board, rows, cols, action_type, tr, tc):\n    \"\"\"Check if a move can be logically deduced from board constraints.\"\"\"\n    cf = set()  # certain flags\n    cr = set()  # certain reveals\n\n    # Phase 1: Single-cell constraint propagation\n    changed = True\n    while changed:\n        changed = False\n        for r in range(rows):\n            for c in range(cols):\n                if board[r][c] not in '12345678':\n                    continue\n                num = int(board[r][c])\n                nbrs = get_neighbors(r, c, rows, cols)\n                fn = sum(1 for nr, nc in nbrs if board[nr][nc] == 'F' or (nr, nc) in cf)\n                un = [(nr, nc) for nr, nc in nbrs\n                      if board[nr][nc] == '.' and (nr, nc) not in cf and (nr, nc) not in cr]\n                rem = num - fn\n                if rem < 0:\n                    continue\n                if rem == len(un) and un:\n                    for n in un:\n                        if n not in cf:\n                            cf.add(n)\n                            changed = True\n                if rem == 0 and un:\n                    for n in un:\n                        if n not in cr:\n                            cr.add(n)\n                            changed = True\n\n    # Phase 2: Coupled constraints (pair-wise subset analysis)\n    numbered = [(r, c) for r in range(rows) for c in range(cols) if board[r][c] in '12345678']\n    changed = True\n    iters = 0\n    while changed and iters < 30:\n        changed = False\n        iters += 1\n        for i, (r1, c1) in enumerate(numbered):\n            n1 = int(board[r1][c1])\n            nb1 = get_neighbors(r1, c1, rows, cols)\n            f1 = sum(1 for nr, nc in nb1 if board[nr][nc] == 'F' or (nr, nc) in cf)\n            u1 = set(n for n in nb1 if board[n[0]][n[1]] == '.' and n not in cf and n not in cr)\n            rm1 = n1 - f1\n            if not u1:\n                continue\n            for j in range(i + 1, len(numbered)):\n                r2, c2 = numbered[j]\n                if abs(r1 - r2) > 2 or abs(c1 - c2) > 2:\n                    continue\n                n2 = int(board[r2][c2])\n                nb2 = get_neighbors(r2, c2, rows, cols)\n                f2 = sum(1 for nr, nc in nb2 if board[nr][nc] == 'F' or (nr, nc) in cf)\n                u2 = set(n for n in nb2 if board[n[0]][n[1]] == '.' and n not in cf and n not in cr)\n                rm2 = n2 - f2\n                if not u2:\n                    continue\n                for sa, sb, ra, rb in [(u1, u2, rm1, rm2), (u2, u1, rm2, rm1)]:\n                    if sa.issubset(sb):\n                        diff = sb - sa\n                        dm = rb - ra\n                        if diff and dm == len(diff):\n                            for cell in diff:\n                                if cell not in cf:\n                                    cf.add(cell)\n                                    changed = True\n                        elif diff and dm == 0:\n                            for cell in diff:\n                                if cell not in cr:\n                                    cr.add(cell)\n                                    changed = True\n\n    target = (tr, tc)\n    return (action_type == \"flag\" and target in cf) or (action_type == \"reveal\" and target in cr)\n\n\nclass MinesweeperSolver:\n    \"\"\"Expert solver using constraint propagation + coupled constraints.\"\"\"\n\n    def analyze_board(self, board, rows, cols, num_mines, num_flagged):\n        cf = set()\n        cr = set()\n        frontier = [(r, c) for r in range(rows) for c in range(cols)\n                     if board[r][c] in '12345678'\n                     and any(board[nr][nc] == '.' for nr, nc in get_neighbors(r, c, rows, cols))]\n\n        # Phase 1: Single-cell constraints\n        changed = True\n        while changed:\n            changed = False\n            for r, c in frontier:\n                num = int(board[r][c])\n                nbrs = get_neighbors(r, c, rows, cols)\n                fn = [n for n in nbrs if board[n[0]][n[1]] == 'F' or n in cf]\n                un = [n for n in nbrs if board[n[0]][n[1]] == '.' and n not in cf and n not in cr]\n                rem = num - len(fn)\n                if rem < 0:\n                    continue\n                if rem == len(un) and un:\n                    for n in un:\n                        if n not in cf:\n                            cf.add(n)\n                            changed = True\n                if rem == 0 and un:\n                    for n in un:\n                        if n not in cr:\n                            cr.add(n)\n                            changed = True\n\n        # Phase 2: Coupled constraints with spatial grid indexing (fast on 50x50)\n        gi = defaultdict(list)\n        for r, c in frontier:\n            gi[(r // 3, c // 3)].append((r, c))\n        changed = True\n        it = 0\n        while changed and it < 50:\n            changed = False\n            it += 1\n            for (gr, gc), fc in gi.items():\n                nearby = []\n                for dr in [-1, 0, 1]:\n                    for dc in [-1, 0, 1]:\n                        nearby.extend(gi.get((gr + dr, gc + dc), []))\n                for r1, c1 in fc:\n                    n1 = int(board[r1][c1])\n                    nb1 = get_neighbors(r1, c1, rows, cols)\n                    f1 = sum(1 for n in nb1 if board[n[0]][n[1]] == 'F' or n in cf)\n                    u1 = set(n for n in nb1 if board[n[0]][n[1]] == '.' and n not in cf and n not in cr)\n                    rm1 = n1 - f1\n                    if not u1:\n                        continue\n                    for r2, c2 in nearby:\n                        if (r1, c1) >= (r2, c2) or abs(r1 - r2) > 2 or abs(c1 - c2) > 2:\n                            continue\n                        n2 = int(board[r2][c2])\n                        nb2 = get_neighbors(r2, c2, rows, cols)\n                        f2 = sum(1 for n in nb2 if board[n[0]][n[1]] == 'F' or n in cf)\n                        u2 = set(n for n in nb2 if board[n[0]][n[1]] == '.' and n not in cf and n not in cr)\n                        rm2 = n2 - f2\n                        if not u2:\n                            continue\n                        for sa, sb, ra, rb in [(u1, u2, rm1, rm2), (u2, u1, rm2, rm1)]:\n                            if sa.issubset(sb):\n                                diff = sb - sa\n                                dm = rb - ra\n                                if diff and dm == len(diff):\n                                    for cell in diff:\n                                        if cell not in cf:\n                                            cf.add(cell)\n                                            changed = True\n                                elif diff and dm == 0:\n                                    for cell in diff:\n                                        if cell not in cr:\n                                            cr.add(cell)\n                                            changed = True\n        cf -= cr\n        return {\"certain_flags\": cf, \"certain_reveals\": cr}\n\n    def estimate_probabilities(self, board, rows, cols, num_mines, cf, cr):\n        cur_flags = sum(1 for r in range(rows) for c in range(cols) if board[r][c] == 'F')\n        rem_mines = max(0, num_mines - cur_flags - len(cf))\n        uncertain = set()\n        for r in range(rows):\n            for c in range(cols):\n                if board[r][c] == '.' and (r, c) not in cf and (r, c) not in cr:\n                    uncertain.add((r, c))\n        if not uncertain:\n            return {}\n        gp = rem_mines / len(uncertain) if uncertain else 0\n        cp = defaultdict(list)\n        for r in range(rows):\n            for c in range(cols):\n                if board[r][c] not in '12345678':\n                    continue\n                num = int(board[r][c])\n                nbrs = get_neighbors(r, c, rows, cols)\n                fn = sum(1 for n in nbrs if board[n[0]][n[1]] == 'F' or n in cf)\n                un = [n for n in nbrs if n in uncertain]\n                if un:\n                    lp = max(0, min(1, (num - fn) / len(un)))\n                    for n in un:\n                        cp[n].append(lp)\n        probs = {}\n        for cell in uncertain:\n            probs[cell] = sum(cp[cell]) / len(cp[cell]) if cell in cp else gp\n        return probs\n\n    def get_best_action(self, game):\n        board = game.get_visible_board()\n        rows, cols = game.rows, game.cols\n        a = self.analyze_board(board, rows, cols, game.num_mines, len(game._flagged))\n        cf, cr = a[\"certain_flags\"], a[\"certain_reveals\"]\n        if cf:\n            r, c = min(cf)\n            return {\"type\": \"flag\", \"row\": r, \"col\": c}, True\n        if cr:\n            r, c = min(cr)\n            return {\"type\": \"reveal\", \"row\": r, \"col\": c}, True\n        probs = self.estimate_probabilities(board, rows, cols, game.num_mines, cf, cr)\n        if probs:\n            safest = min(probs.keys(), key=lambda k: (probs[k], k))\n            return {\"type\": \"reveal\", \"row\": safest[0], \"col\": safest[1]}, False\n        for r in range(rows):\n            for c in range(cols):\n                if board[r][c] == '.':\n                    return {\"type\": \"reveal\", \"row\": r, \"col\": c}, False\n        return None, False\n\nsolver = MinesweeperSolver()\n\n# Quick solver test\ntest_game = MinesweeperGame(rows=8, cols=8, num_mines=10, seed=42)\ntest_game.do_action({\"type\": \"reveal\", \"row\": 4, \"col\": 4})\naction, is_logical = solver.get_best_action(test_game)\nprint(f\"Solver recommends: {action} (logical: {is_logical})\")\nprint(f\"\\nCompact prompt ({len(build_compact_prompt(test_game))} chars):\")\nprint(build_compact_prompt(test_game))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Base Model (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n\nFastLanguageModel.for_inference(model)\n\ngame = MinesweeperGame(rows=8, cols=8, num_mines=10, seed=42)\ngame.do_action({\"type\": \"reveal\", \"row\": 4, \"col\": 4})\nprompt = build_compact_prompt(game)\n\nmessages = [\n    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n    {\"role\": \"user\", \"content\": prompt},\n]\ntry:\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\nexcept TypeError:\n    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\nprint(\"=== Base Model Response ===\")\noutput = model.generate(\n    **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n    temperature=1.0, do_sample=True, max_new_tokens=128,\n    streamer=TextStreamer(tokenizer, skip_prompt=True),\n)\nFastLanguageModel.for_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate SFT Training Data\n\n10,000 expert-solved examples across all board sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expert solver generates optimal (state, action) pairs.\n# Variable board sizes 5x5->50x50 for generalization.\n# ################################################################\ndef generate_sft_dataset(num_samples=10000, rng_seed=42):\n    \"\"\"Generate expert-solved training data (logical-only, no guesses).\n    KEY CHANGES from previous version:\n    1. LOGICAL-ONLY: Skip examples where solver had to guess (~30-50% noise removed)\n    2. RANDOMIZED selection: Random choice from cf/cr sets (removes top-left bias)\n    3. More attempts to compensate for logical-only filtering\n    \"\"\"\n    np.random.seed(rng_seed)\n    random.seed(rng_seed)\n\n    dataset_items = []\n    slvr = MinesweeperSolver()\n    skipped_non_logical = 0\n\n    # Board size distribution: (rows, cols, mine_pct, weight)\n    board_configs = [\n        # Square boards\n        (5, 5, 0.12, 0.03), (6, 6, 0.14, 0.05), (8, 8, 0.15, 0.08),\n        (10, 10, 0.15, 0.08), (12, 12, 0.15, 0.05), (16, 16, 0.15, 0.05),\n        (20, 20, 0.15, 0.04), (25, 25, 0.15, 0.02), (30, 30, 0.15, 0.02),\n        (40, 40, 0.12, 0.02), (50, 50, 0.10, 0.02),\n        (50, 50, 0.15, 0.02), (50, 50, 0.20, 0.01),\n        # Rectangular boards (wide)\n        (5, 8, 0.14, 0.03), (6, 10, 0.14, 0.04), (8, 12, 0.15, 0.04),\n        (8, 16, 0.15, 0.03), (10, 16, 0.15, 0.04), (12, 20, 0.15, 0.03),\n        (15, 25, 0.15, 0.03), (20, 30, 0.15, 0.03), (25, 40, 0.15, 0.02),\n        (30, 50, 0.15, 0.02),\n        # Rectangular boards (tall)\n        (8, 5, 0.14, 0.02), (10, 6, 0.14, 0.03), (12, 8, 0.15, 0.03),\n        (16, 10, 0.15, 0.03), (20, 12, 0.15, 0.02), (25, 15, 0.15, 0.02),\n        (30, 20, 0.15, 0.02), (40, 25, 0.12, 0.01), (50, 30, 0.15, 0.01),\n    ]\n\n    total_w = sum(w for _, _, _, w in board_configs)\n    targets = [(r, c, mp, max(1, int(num_samples * w / total_w)))\n               for r, c, mp, w in board_configs]\n\n    for rows, cols, mine_pct, target in targets:\n        mines = max(1, int(rows * cols * mine_pct))\n        gen = 0\n        attempts = 0\n        # More attempts since we filter out non-logical examples\n        while gen < target and attempts < target * 20:\n            attempts += 1\n            seed = np.random.randint(1000000)\n            try:\n                game = MinesweeperGame(rows=rows, cols=cols, num_mines=mines, seed=seed)\n            except ValueError:\n                continue\n\n            # Random first move\n            fr, fc = np.random.randint(0, rows), np.random.randint(0, cols)\n            game.do_action({\"type\": \"reveal\", \"row\": int(fr), \"col\": int(fc)})\n            if game.state() != \"ongoing\":\n                continue\n\n            # Play solver moves to reach a mid/late game state\n            move_history = [{\"type\": \"reveal\", \"row\": int(fr), \"col\": int(fc)}]\n            max_depth = max(min(rows * cols // 2, 40), 4)\n            # Bias toward mid/late game where logical deductions exist\n            r_val = np.random.random()\n            if r_val < 0.1:\n                num_extra = np.random.randint(0, min(3, max_depth))\n            elif r_val < 0.5:\n                num_extra = np.random.randint(2, max(max_depth * 2 // 3, 3))\n            else:\n                num_extra = np.random.randint(max(max_depth // 3, 2), max_depth)\n            for _ in range(num_extra):\n                if game.state() != \"ongoing\":\n                    break\n                act, _ = slvr.get_best_action(game)\n                if act is None:\n                    break\n                game.do_action(act)\n                move_history.append(act)\n\n            if game.state() != \"ongoing\":\n                continue\n\n            # Get solver analysis - use analyze_board directly for random selection\n            board = game.get_visible_board()\n            analysis = slvr.analyze_board(board, rows, cols, mines, len(game._flagged))\n            cf, cr = analysis[\"certain_flags\"], analysis[\"certain_reveals\"]\n\n            # LOGICAL-ONLY: Skip if no certain moves exist\n            if not cf and not cr:\n                skipped_non_logical += 1\n                continue\n\n            # RANDOM SELECTION: Pick random from cf/cr (removes top-left bias)\n            # When both exist, 50/50 between flag and reveal for balanced training\n            if cf and cr:\n                if random.random() < 0.5:\n                    r_act, c_act = random.choice(list(cf))\n                    act = {\"type\": \"flag\", \"row\": r_act, \"col\": c_act}\n                else:\n                    r_act, c_act = random.choice(list(cr))\n                    act = {\"type\": \"reveal\", \"row\": r_act, \"col\": c_act}\n            elif cf:\n                r_act, c_act = random.choice(list(cf))\n                act = {\"type\": \"flag\", \"row\": r_act, \"col\": c_act}\n            else:\n                r_act, c_act = random.choice(list(cr))\n                act = {\"type\": \"reveal\", \"row\": r_act, \"col\": c_act}\n\n            # ADD CHAIN-OF-THOUGHT: include reasoning in JSON output\n            # Research shows 60% accuracy boost from \"Think Inside the JSON\"\n            reasoning = generate_reasoning(board, rows, cols, act)\n            act_with_think = {\"think\": reasoning, **act}\n            response_text = json.dumps(act_with_think)\n\n            prompt_text = build_compact_prompt(game)\n            dataset_items.append({\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": prompt_text},\n                    {\"role\": \"assistant\", \"content\": response_text},\n                ],\n                \"seed\": seed,\n                \"move_history\": json.dumps(move_history),\n                \"game_rows\": rows,\n                \"game_cols\": cols,\n                \"game_mines\": mines,\n            })\n            gen += 1\n\n        print(f\"  {rows}x{cols} ({mines} mines): {gen}/{target} examples\")\n\n    random.shuffle(dataset_items)\n    print(f\"\\nSkipped {skipped_non_logical} non-logical examples (noise removed)\")\n    return Dataset.from_list(dataset_items)\n\nprint(\"Generating SFT dataset (logical-only, balanced flag/reveal)...\")\nsft_dataset = generate_sft_dataset(num_samples=3000)\nprint(f\"\\nGenerated {len(sft_dataset)} SFT examples (all logically deducible)\")\n\n# Show distribution\nsizes = defaultdict(int)\nfor item in sft_dataset:\n    sizes[f\"{item['game_rows']}x{item['game_cols']}\"] += 1\nprint(\"\\nBoard size distribution:\")\nfor s, c in sorted(sizes.items(), key=lambda x: int(x[0].split('x')[0])):\n    print(f\"  {s}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFT Training (2 epochs for non-instruct model)\n\nMore epochs needed since gpt-oss-20b is not instruction-tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teaches the model:\n# 1. Output format: pure JSON action (no reasoning text)\n# 2. Constraint-based minesweeper logic (logical-only examples, no guesses)\n# 3. When to flag vs reveal based on neighbor constraints\n# ################################################################\nfrom trl import SFTConfig, SFTTrainer\n\n# Pre-format dataset: apply chat template to create a \"text\" column\n# This avoids Unsloth's formatting_func quirks with batched tokenization\ndef _format_to_text(example):\n    try:\n        text = tokenizer.apply_chat_template(\n            example[\"messages\"], tokenize=False,\n            add_generation_prompt=False, enable_thinking=False\n        )\n    except TypeError:\n        text = tokenizer.apply_chat_template(\n            example[\"messages\"], tokenize=False,\n            add_generation_prompt=False\n        )\n    return {\"text\": text}\n\nsft_dataset = sft_dataset.map(_format_to_text)\nprint(f\"Sample formatted text (first 300 chars):\\n{sft_dataset[0]['text'][:300]}\")\n\nsft_config = SFTConfig(\n    output_dir=\"minesweeper_sft_output\",\n    per_device_train_batch_size=8,       # 256GB GPU can handle large batches\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,                     # 1 epoch to fit in 1-hour time budget (~19 min)\n    learning_rate=2e-5,\n    weight_decay=0.01,\n    warmup_ratio=0.1,\n    lr_scheduler_type=\"cosine\",\n    logging_steps=10,\n    save_steps=500,\n    max_seq_length=max_seq_length,\n    optim=\"adamw_8bit\",\n    report_to=\"none\",\n    dataset_text_field=\"text\",\n)\n\nsft_trainer = SFTTrainer(\n    model=model,\n    processing_class=tokenizer,\n    train_dataset=sft_dataset,\n    args=sft_config,\n)\n\nprint(\"Starting SFT training...\")\nsft_trainer.train()\nprint(\"SFT training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save SFT Checkpoint + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"minesweeper_sft_checkpoint_oss\")\ntokenizer.save_pretrained(\"minesweeper_sft_checkpoint_oss\")\nprint(\"SFT checkpoint saved\")\n\ndef quick_eval(model, tokenizer, num_games=10, label=\"\"):\n    \"\"\"Score-based eval matching competition rules. Game continues after non-fatal errors.\"\"\"\n    FastLanguageModel.for_inference(model)\n    wins = 0\n    valid_json = 0\n    total_moves = 0\n    total_score = 0.0\n    for seed_i in range(num_games):\n        game = MinesweeperGame(rows=8, cols=8, num_mines=10, seed=seed_i + 10000)\n        game.do_action({\"type\": \"reveal\", \"row\": 4, \"col\": 4})\n        moves = 0\n        game_score = 0.0\n        consecutive_bad = 0\n        while game.state() == \"ongoing\" and moves < 100 and consecutive_bad < 5:\n            prompt = build_compact_prompt(game)\n            msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": prompt}]\n            try:\n                text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                    add_generation_prompt=True, enable_thinking=False)\n            except TypeError:\n                text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                    add_generation_prompt=True)\n            inp = tokenizer(text, return_tensors=\"pt\").to(model.device)\n            out = model.generate(**inp, temperature=0.3, max_new_tokens=128, do_sample=True)\n            resp = tokenizer.decode(out[0][inp.input_ids.shape[1]:], skip_special_tokens=True)\n            action = parse_llm_action(resp)\n            moves += 1\n            if action is None:\n                game_score -= 10.0\n                consecutive_bad += 1\n                continue\n            valid_json += 1\n            row, col = int(action[\"row\"]), int(action[\"col\"])\n            atype = action[\"type\"]\n            # Pre-check action without breaking game state\n            if not (0 <= row < 8 and 0 <= col < 8):\n                game_score -= 15.0; consecutive_bad += 1; continue\n            if atype == \"reveal\":\n                if (row, col) in game._revealed:\n                    game_score -= 12.0; consecutive_bad += 1; continue\n                if (row, col) in game._flagged:\n                    game_score -= 8.0; consecutive_bad += 1; continue\n                if game._board[row][col] == -1:\n                    game_score -= 25.0; break  # Mine = game over\n                consecutive_bad = 0\n                board = game.get_visible_board()\n                is_log = is_logically_deducible(board, 8, 8, \"reveal\", row, col)\n                game_score += 15.0 if is_log else 10.0\n                game.do_action(action)\n                if game.state() == \"success\":\n                    game_score += 100.0\n            elif atype == \"flag\":\n                if (row, col) in game._revealed:\n                    game_score -= 8.0; consecutive_bad += 1; continue\n                if (row, col) in game._flagged:\n                    game_score -= 8.0; consecutive_bad += 1; continue\n                consecutive_bad = 0\n                if len(game._flagged) + 1 > 10:\n                    game_score -= 10.0\n                if game._board[row][col] == -1:\n                    game_score += 15.0\n                else:\n                    game_score -= 10.0\n                game.do_action(action)\n        total_moves += moves\n        total_score += game_score\n        if game.state() == \"success\":\n            wins += 1\n    FastLanguageModel.for_training(model)\n    avg_score = total_score / num_games\n    print(f\"[{label}] {wins}/{num_games} wins, {valid_json} valid JSON, \"\n          f\"avg {total_moves/num_games:.1f} moves/game, avg score {avg_score:.1f}\")\n\nprint(\"\\nEvaluating SFT model...\")\nquick_eval(model, tokenizer, num_games=3, label=\"Post-SFT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Reward Functions (v3 - Lessons from Qwen)\n\n**Key fix: frontier bonus no longer cancels random penalty**\n- Mine hit: **-100** (4x penalty)\n- Random reveal: **-15** (no frontier bonus!)\n- Logical reveal: **+30** (+5 frontier bonus)\n- Correct flag: **+30**\n- Invalid JSON: **-25** (harsh)\n- Win: **+200**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Format reward: Valid JSON output, bonus for conciseness\n# 2. Gameplay reward: All 12 scoring criteria + logical deduction\n# 3. Conciseness reward: Penalizes verbose output (128 token limit)\n# ################################################################\ndef valid_json_reward(completions, **kwargs):\n    \"\"\"Strict JSON format reward - harsh invalid penalty.\"\"\"\n    scores = []\n    for completion in completions:\n        try:\n            response = completion[0][\"content\"] if isinstance(completion, list) else str(completion)\n        except Exception:\n            scores.append(-25.0)\n            continue\n        action = parse_llm_action(response)\n        if action is None:\n            scores.append(-25.0)\n        else:\n            stripped = response.strip()\n            if stripped.startswith('{') and len(stripped) < 150:\n                scores.append(8.0)    # JSON with think field (~100-130 chars)\n            elif stripped.startswith('{'):\n                scores.append(5.0)\n            else:\n                scores.append(1.0)\n    return scores\n\ndef gameplay_reward(completions, **kwargs):\n    \"\"\"Improved gameplay reward v3 - lessons from Qwen Phase 2 GRPO.\n    Key fix: frontier bonus was CANCELING random penalty (-5+5=0).\n    Now: frontier bonus ONLY for logical reveals. Random = always punished.\n    Wider gap between logical (+30) and random (-15) reveals.\"\"\"\n    scores = []\n    seeds = kwargs.get(\"seed\", [])\n    mh_list = kwargs.get(\"move_history\", [])\n    gr_list = kwargs.get(\"game_rows\", [])\n    gc_list = kwargs.get(\"game_cols\", [])\n    gm_list = kwargs.get(\"game_mines\", [])\n\n    # Ensure lists (some trl versions pass scalars for batch=1)\n    if not isinstance(seeds, (list, tuple)):\n        seeds = [seeds]\n    if not isinstance(mh_list, (list, tuple)):\n        mh_list = [mh_list]\n    if not isinstance(gr_list, (list, tuple)):\n        gr_list = [gr_list]\n    if not isinstance(gc_list, (list, tuple)):\n        gc_list = [gc_list]\n    if not isinstance(gm_list, (list, tuple)):\n        gm_list = [gm_list]\n\n    for idx, completion in enumerate(completions):\n        try:\n            response = completion[0][\"content\"] if isinstance(completion, list) else str(completion)\n        except Exception:\n            scores.append(-10.0)\n            continue\n        action = parse_llm_action(response)\n        if action is None:\n            scores.append(-10.0)\n            continue\n        if not seeds or not mh_list:\n            scores.append(0.0)\n            continue\n\n        # Handle both repeated and non-repeated kwargs across trl versions\n        pi = idx % max(1, len(seeds))\n        seed = seeds[pi]\n        mh_raw = mh_list[pi % max(1, len(mh_list))]\n        rows = gr_list[pi % max(1, len(gr_list))] if gr_list else 6\n        cols = gc_list[pi % max(1, len(gc_list))] if gc_list else 6\n        mines = gm_list[pi % max(1, len(gm_list))] if gm_list else 5\n        mh = json.loads(mh_raw) if isinstance(mh_raw, str) else mh_raw\n\n        game = MinesweeperGame(rows=rows, cols=cols, num_mines=mines, seed=seed)\n        for prev in mh:\n            game.do_action(prev)\n        if game.state() != \"ongoing\":\n            scores.append(0.0)\n            continue\n\n        board = game.get_visible_board()\n        try:\n            row, col = int(action[\"row\"]), int(action[\"col\"])\n        except (ValueError, TypeError):\n            scores.append(-10.0)\n            continue\n\n        atype = action[\"type\"]\n\n        # Criterion 7: Out of bounds -> -20\n        if not (0 <= row < rows and 0 <= col < cols):\n            scores.append(-20.0)\n            continue\n\n        score = 0.0\n        if atype == \"reveal\":\n            # Criterion 6: Already revealed -> -15\n            if (row, col) in game._revealed:\n                scores.append(-15.0)\n                continue\n            # Criterion 11: Reveal flagged cell -> -10\n            if (row, col) in game._flagged:\n                scores.append(-10.0)\n                continue\n            # Criterion 3: Reveal mine -> -100 (4x actual, MUST avoid mines)\n            if game._board[row][col] == -1:\n                score = -100.0\n            else:\n                # Criterion 4: Reveal safe\n                is_log = is_logically_deducible(board, rows, cols, \"reveal\", row, col)\n                if is_log:\n                    # Logical reveal -> +30 base\n                    score = 30.0\n                    # Frontier bonus ONLY for logical reveals (+5)\n                    nbrs = get_neighbors(row, col, rows, cols)\n                    near_revealed = any((nr, nc) in game._revealed for nr, nc in nbrs)\n                    if near_revealed:\n                        score += 5.0\n                else:\n                    # Random reveal -> -15 ALWAYS (no frontier bonus!)\n                    # This is the KEY fix: Qwen's -5+5=0 gave no signal\n                    score = -15.0\n                # Criterion 10: Check win bonus +200\n                tg = MinesweeperGame(rows=rows, cols=cols, num_mines=mines, seed=seed)\n                for prev in mh:\n                    tg.do_action(prev)\n                tg.do_action(action)\n                if tg.state() == \"success\":\n                    score += 200.0\n        elif atype == \"flag\":\n            # Criterion 12: Flag revealed cell -> -10\n            if (row, col) in game._revealed:\n                scores.append(-10.0)\n                continue\n            # Criterion 5: Flag already flagged -> -10\n            if (row, col) in game._flagged:\n                scores.append(-10.0)\n                continue\n            # Criterion 8: Too many flags -> -15\n            if len(game._flagged) + 1 > mines:\n                score -= 15.0\n            # Criterion 1: Flag mine -> +30, Criterion 2: Flag non-mine -> -10\n            if game._board[row][col] == -1:\n                score += 30.0\n            else:\n                score -= 10.0\n\n        scores.append(score)\n    return scores\n\ndef conciseness_reward(completions, **kwargs):\n    \"\"\"Reward concise pure JSON output. High rewards prevent format degradation.\"\"\"\n    scores = []\n    for completion in completions:\n        try:\n            response = completion[0][\"content\"] if isinstance(completion, list) else str(completion)\n        except Exception:\n            scores.append(-3.0)\n            continue\n        stripped = response.strip()\n        action = parse_llm_action(response)\n        if action is None:\n            scores.append(-3.0)\n            continue\n        tok_est = len(stripped) / 4\n        if stripped.startswith('{') and tok_est < 40:\n            scores.append(5.0)     # Perfect: pure JSON with think field (~30 tokens)\n        elif stripped.startswith('{') and tok_est < 60:\n            scores.append(2.5)     # Good: starts with JSON\n        elif tok_est < 80:\n            scores.append(1.0)     # Acceptable\n        else:\n            scores.append(-3.0)    # Too verbose\n    return scores\n\nprint(\"Reward functions defined (3 functions, 12 criteria)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate GRPO Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to SFT but prompt-only (no expert answer).\n# Model explores, reward functions guide learning.\n# ################################################################\ndef generate_grpo_dataset(num_samples=5000, rng_seed=123):\n    \"\"\"Generate diverse game states for GRPO training.\n    Includes rectangular boards and varied game depths.\"\"\"\n    np.random.seed(rng_seed)\n    random.seed(rng_seed)\n    items = []\n    slvr = MinesweeperSolver()\n\n    configs = [\n        # Square boards\n        (5, 5, 0.12, 0.03), (6, 6, 0.14, 0.05), (8, 8, 0.15, 0.08),\n        (10, 10, 0.15, 0.08), (12, 12, 0.15, 0.05), (16, 16, 0.15, 0.05),\n        (20, 20, 0.15, 0.04), (25, 25, 0.15, 0.02), (30, 30, 0.15, 0.02),\n        (40, 40, 0.12, 0.02), (50, 50, 0.10, 0.02),\n        (50, 50, 0.15, 0.02), (50, 50, 0.20, 0.01),\n        # Rectangular boards\n        (5, 8, 0.14, 0.03), (6, 10, 0.14, 0.04), (8, 12, 0.15, 0.04),\n        (8, 16, 0.15, 0.03), (10, 16, 0.15, 0.04), (12, 20, 0.15, 0.03),\n        (15, 25, 0.15, 0.03), (20, 30, 0.15, 0.02), (25, 40, 0.15, 0.02),\n        (30, 50, 0.15, 0.02),\n        (10, 6, 0.14, 0.03), (16, 10, 0.15, 0.03), (20, 12, 0.15, 0.02),\n        (50, 30, 0.15, 0.01), (40, 25, 0.12, 0.01),\n    ]\n    total_w = sum(w for _, _, _, w in configs)\n\n    for rows, cols, mp, weight in configs:\n        mines = max(1, int(rows * cols * mp))\n        target = max(1, int(num_samples * weight / total_w))\n        gen = 0\n        attempts = 0\n        while gen < target and attempts < target * 10:\n            attempts += 1\n            seed = np.random.randint(1000000)\n            try:\n                game = MinesweeperGame(rows=rows, cols=cols, num_mines=mines, seed=seed)\n            except ValueError:\n                continue\n            fr, fc = np.random.randint(0, rows), np.random.randint(0, cols)\n            fa = {\"type\": \"reveal\", \"row\": int(fr), \"col\": int(fc)}\n            game.do_action(fa)\n            if game.state() != \"ongoing\":\n                continue\n            mh = [fa]\n            max_depth = max(min(rows * cols // 2, 40), 4)\n            # Bias toward mid/late game where logical deductions exist\n            # (matches SFT distribution for better reward variance in GRPO)\n            r_val = np.random.random()\n            if r_val < 0.1:\n                num_extra = np.random.randint(0, min(3, max_depth))\n            elif r_val < 0.5:\n                num_extra = np.random.randint(2, max(max_depth * 2 // 3, 3))\n            else:\n                num_extra = np.random.randint(max(max_depth // 3, 2), max_depth)\n            for _ in range(num_extra):\n                if game.state() != \"ongoing\":\n                    break\n                act, _ = slvr.get_best_action(game)\n                if act is None:\n                    break\n                game.do_action(act)\n                mh.append(act)\n            if game.state() != \"ongoing\":\n                continue\n\n            items.append({\n                \"prompt\": [\n                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                    {\"role\": \"user\", \"content\": build_compact_prompt(game)},\n                ],\n                \"seed\": seed,\n                \"move_history\": json.dumps(mh),\n                \"game_rows\": rows,\n                \"game_cols\": cols,\n                \"game_mines\": mines,\n            })\n            gen += 1\n        print(f\"  {rows}x{cols} ({mines} mines): {gen}/{target}\")\n\n    random.shuffle(items)\n    return Dataset.from_list(items)\n\nprint(\"Generating GRPO dataset...\")\ngrpo_dataset = generate_grpo_dataset(num_samples=2000)\nprint(f\"Generated {len(grpo_dataset)} GRPO examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Training (v3)\n\n- `beta=0.04`, `temperature=0.7`\n- `learning_rate=5e-6`, `max_grad_norm=0.5`\n- 300 steps (SFT with CoT is the main driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIX for training_loss=0: Explicitly ensure model is in training mode,\n# use non-zero beta, and verify trainable parameters before starting.\n# ################################################################\nfrom trl import GRPOConfig, GRPOTrainer\nimport inspect\n\n# CRITICAL: Ensure model is in training mode with gradients\n# (Unsloth's for_inference from eval cell may persist)\nFastLanguageModel.for_training(model)\nmodel.train()\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Trainable: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")\nassert trainable_params > 0, \"ERROR: No trainable parameters! LoRA may not be attached.\"\n\n# Build GRPO config - max_prompt_length may not exist in all trl versions\n_grpo_kwargs = dict(\n    # Generation - lower temp since Qwen showed too-random exploration was wasteful\n    temperature=0.7,\n    num_generations=4,            # Reduced from 8 to save time (halves generation cost)\n\n    # Optimizer\n    learning_rate=5e-6,\n    weight_decay=0.1,\n    warmup_ratio=0.15,\n    lr_scheduler_type=\"cosine\",\n    optim=\"adamw_8bit\",\n    max_grad_norm=0.5,\n\n    # Batching\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=2,\n\n    # Lengths\n    max_completion_length=128,\n\n    # Training duration - 1 hour total budget, ~20 min for GRPO\n    max_steps=100,\n    save_steps=50,\n\n    # GRPO specific - use DAPO-style (beta=0.0) with loss_type\n    # Qwen's beta=0.3 was too conservative, model barely moved\n    beta=0.04,\n\n    # Logging\n    logging_steps=1,\n    report_to=\"none\",\n    output_dir=\"minesweeper_grpo_output\",\n)\n# Only add max_prompt_length if this trl version supports it\nif \"max_prompt_length\" in inspect.signature(GRPOConfig).parameters:\n    _grpo_kwargs[\"max_prompt_length\"] = 3500\ngrpo_config = GRPOConfig(**_grpo_kwargs)\n\ngrpo_trainer = GRPOTrainer(\n    model=model,\n    processing_class=tokenizer,\n    reward_funcs=[valid_json_reward, gameplay_reward, conciseness_reward],\n    args=grpo_config,\n    train_dataset=grpo_dataset,\n)\n\nprint(\"Starting GRPO training...\")\nprint(\"NOTE: Rewards may not improve for first ~100 steps - this is NORMAL!\")\ngrpo_trainer.train()\nprint(\"GRPO training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_full_game(model, tokenizer, rows=8, cols=8, num_mines=10, seed=None, max_moves=200):\n    \"\"\"Play a full game with competition-style scoring.\n    Game continues after non-fatal errors (only mine hit ends the game).\"\"\"\n    game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n    game.do_action({\"type\": \"reveal\", \"row\": rows // 2, \"col\": cols // 2})\n    moves = 0\n    score = 0.0\n    consecutive_bad = 0\n    while game.state() == \"ongoing\" and moves < max_moves and consecutive_bad < 5:\n        prompt = build_compact_prompt(game)\n        msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt}]\n        try:\n            text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                add_generation_prompt=True, enable_thinking=False)\n        except TypeError:\n            text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                add_generation_prompt=True)\n        inp = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        out = model.generate(**inp, temperature=0.3, max_new_tokens=128, do_sample=True)\n        resp = tokenizer.decode(out[0][inp.input_ids.shape[1]:], skip_special_tokens=True)\n        action = parse_llm_action(resp)\n        moves += 1\n        if action is None:\n            score -= 10.0; consecutive_bad += 1; continue\n        try:\n            row, col = int(action[\"row\"]), int(action[\"col\"])\n        except (ValueError, TypeError):\n            score -= 10.0; consecutive_bad += 1; continue\n        atype = action[\"type\"]\n        # Pre-check: don't let bad moves kill game state\n        if not (0 <= row < rows and 0 <= col < cols):\n            score -= 15.0; consecutive_bad += 1; continue\n        if atype == \"reveal\":\n            if (row, col) in game._revealed:\n                score -= 12.0; consecutive_bad += 1; continue\n            if (row, col) in game._flagged:\n                score -= 8.0; consecutive_bad += 1; continue\n            if game._board[row][col] == -1:\n                score -= 25.0; break  # Mine = game over\n            consecutive_bad = 0\n            board = game.get_visible_board()\n            is_log = is_logically_deducible(board, rows, cols, \"reveal\", row, col)\n            score += 15.0 if is_log else 10.0\n            game.do_action(action)\n            if game.state() == \"success\":\n                score += 100.0\n        elif atype == \"flag\":\n            if (row, col) in game._revealed:\n                score -= 8.0; consecutive_bad += 1; continue\n            if (row, col) in game._flagged:\n                score -= 8.0; consecutive_bad += 1; continue\n            consecutive_bad = 0\n            if len(game._flagged) + 1 > num_mines:\n                score -= 10.0\n            if game._board[row][col] == -1:\n                score += 15.0\n            else:\n                score -= 10.0\n            game.do_action(action)\n    return game, moves, score\n\nFastLanguageModel.for_inference(model)\n\neval_configs = [\n    # Quick eval - 3 games each to save time\n    (8, 8, 10, 3, \"8x8\"),\n    (10, 10, 15, 3, \"10x10\"),\n    (6, 10, 8, 3, \"6x10\"),\n]\n\nprint(\"=\" * 60)\nprint(\"FINAL EVALUATION (competition-style scoring)\")\nprint(\"=\" * 60)\nfor rows, cols, mines, num_games, label in eval_configs:\n    wins = 0\n    total_moves = 0\n    total_score = 0.0\n    for i in range(num_games):\n        game, moves, sc = play_full_game(model, tokenizer, rows, cols, mines,\n                                         seed=20000 + i, max_moves=2 * rows * cols)\n        if game.state() == \"success\":\n            wins += 1\n        total_moves += moves\n        total_score += sc\n    avg_sc = total_score / num_games\n    print(f\"{label}: {wins}/{num_games} wins ({wins/num_games*100:.0f}%), \"\n          f\"avg {total_moves/num_games:.1f} moves, avg score {avg_sc:.1f}\")\nprint(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model\n\nSaved to `your_fine_tuned_model_oss/` (separate from Qwen model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\nmodel.save_pretrained(\"my_minesweeper_model_oss\")\ntokenizer.save_pretrained(\"my_minesweeper_model_oss\")\nprint(\"LoRA adapters saved to: my_minesweeper_model_oss/\")\n\n# Save merged model (this is what the inference agent loads)\n# FIX: Unsloth bug - cache dir exists but lacks permissions, causing\n# UnboundLocalError on 'copied_tokenizer_model_from_cache'.\n# Workaround: point HF_HOME to a writable directory.\n_old_hf_home = os.environ.get(\"HF_HOME\", \"\")\nos.environ[\"HF_HOME\"] = \"/workspace/hf_cache\"\nos.makedirs(\"/workspace/hf_cache\", exist_ok=True)\ntry:\n    model.save_pretrained_merged(\n        \"your_fine_tuned_model_oss\",\n        tokenizer,\n        save_method=\"merged_16bit\"\n    )\n    print(\"Merged model saved to: your_fine_tuned_model_oss/\")\nexcept Exception as e:\n    print(f\"save_pretrained_merged failed: {e}\")\n    print(\"Falling back to manual merge + save...\")\n    model = model.merge_and_unload()\n    model.save_pretrained(\"your_fine_tuned_model_oss\")\n    tokenizer.save_pretrained(\"your_fine_tuned_model_oss\")\n    print(\"Merged model saved (manual fallback) to: your_fine_tuned_model_oss/\")\nfinally:\n    os.environ[\"HF_HOME\"] = _old_hf_home\n\n# Also copy to /workspace path that the agent expects\nimport shutil\nsrc = os.path.abspath(\"your_fine_tuned_model_oss\")\ndst = \"/workspace/your_fine_tuned_model_oss\"\nif src != dst and not os.path.exists(dst):\n    try:\n        os.symlink(src, dst)\n        print(f\"Symlinked {src} -> {dst}\")\n    except Exception as e:\n        print(f\"Note: Could not symlink to {dst}: {e}\")\n        print(f\"Model is at: {src}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Agent Files\n\nPoints to gpt-oss-20b model with constraint-logic prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRITICAL: The inference agent's prompt format MUST match training.\n# This cell writes the updated agent files for evaluation.\n# ################################################################\n\n# --- Write agents/minesweeper_agent.py (clean - no post-processing) ---\nAGENT_CODE = r'''#!/usr/bin/python3\n\"\"\"Minesweeper Agent - Competition Version (no post-processing)\"\"\"\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import Dict, Any, Optional\nfrom .minesweeper_model import MinesweeperAgent\n\n\nclass MinesweeperPlayer:\n    \"\"\"Agent responsible for playing Minesweeper.\n    Only uses: prompt engineering + JSON parsing. No post-processing.\"\"\"\n\n    def __init__(self, **kwargs):\n        self.agent = MinesweeperAgent(**kwargs)\n\n    def build_prompt(self, game_state: Dict[str, Any]) -> tuple:\n        board = game_state[\"board\"]\n        rows = game_state[\"rows\"]\n        cols = game_state[\"cols\"]\n        mines = game_state[\"mines\"]\n        flagged = game_state.get(\"flags_placed\", 0)\n        revealed = game_state.get(\"cells_revealed\", 0)\n\n        board_lines = []\n        for r in range(rows):\n            board_lines.append(f\"{r:>2}|{''.join(board[r])}\")\n        board_str = \"\\n\".join(board_lines)\n\n        prompt = f\"\"\"Minesweeper {rows}x{cols}, {mines} mines, {flagged} flagged, {revealed} revealed.\n.=unknown F=flag 0-8=adjacent mines\n\n{board_str}\n\nJSON action:\"\"\"\n\n        sys_prompt = 'Analyze the Minesweeper board. CRITICAL: You can ONLY target cells marked \".\" (unknown). NEVER pick a numbered cell (0-8) or flagged cell (F) - those are already revealed/flagged. For each numbered cell, count adjacent flags(F) and unknowns(.). If number equals flag count, unknowns are safe to reveal. If number minus flags equals unknown count, unknowns are mines to flag. Only act on certain deductions. Output JSON: {\"think\":\"<brief constraint reasoning>\",\"type\":\"reveal\"or\"flag\",\"row\":N,\"col\":N}'\n        return prompt, sys_prompt\n\n    def play_action(self, game_state, **gen_kwargs):\n        prompt, sys_prompt = self.build_prompt(game_state)\n        response, tl, gt = self.agent.generate_response(prompt, sys_prompt, **gen_kwargs)\n        action = self.parse_action(response)\n        return action, tl, gt\n\n    def parse_action(self, response: str) -> Optional[Dict]:\n        try:\n            potential_jsons = []\n            i = 0\n            while i < len(response):\n                start = response.find(\"{\", i)\n                if start == -1:\n                    break\n                brace_count = 0\n                end = start\n                while end < len(response):\n                    if response[end] == '{':\n                        brace_count += 1\n                    elif response[end] == '}':\n                        brace_count -= 1\n                        if brace_count == 0:\n                            json_str = response[start:end+1]\n                            try:\n                                obj = json.loads(json_str)\n                                potential_jsons.append(obj)\n                            except:\n                                pass\n                            break\n                    end += 1\n                i = end + 1 if end < len(response) else len(response)\n\n            for obj in potential_jsons:\n                if (isinstance(obj, dict) and\n                    \"type\" in obj and \"row\" in obj and \"col\" in obj and\n                    obj[\"type\"] in [\"reveal\", \"flag\"]):\n                    obj[\"row\"] = int(obj[\"row\"])\n                    obj[\"col\"] = int(obj[\"col\"])\n                    return obj\n        except Exception as e:\n            print(f\"Failed to parse action: {e}\")\n            return None\n        return None\n\n    @staticmethod\n    def save_action(action: Dict, file_path) -> None:\n        file_path = Path(file_path)\n        file_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(file_path, \"w\") as f:\n            json.dump(action, f, indent=2)\n\n\nif __name__ == \"__main__\":\n    import argparse\n    import yaml\n\n    argparser = argparse.ArgumentParser(description=\"Play Minesweeper using fine-tuned LLM.\")\n    argparser.add_argument(\"--game_state_file\", type=str, required=True)\n    argparser.add_argument(\"--output_file\", type=str, default=\"outputs/action.json\")\n    argparser.add_argument(\"--verbose\", action=\"store_true\")\n    args = argparser.parse_args()\n\n    with open(args.game_state_file, \"r\") as f:\n        game_state = json.load(f)\n\n    player = MinesweeperPlayer()\n    gen_kwargs = {\"tgps_show\": args.verbose}\n    config_file = Path(\"minesweeper_config.yaml\")\n    if config_file.exists():\n        with open(config_file, \"r\") as f:\n            gen_kwargs.update(yaml.safe_load(f))\n\n    action, tl, gt = player.play_action(game_state, **gen_kwargs)\n    if args.verbose:\n        print(f\"Generated Action: {json.dumps(action, indent=2)}\")\n    if action:\n        player.save_action(action, args.output_file)\n        print(f\"Action saved to {args.output_file}\")\n    else:\n        print(\"ERROR: Failed to generate valid action!\")\n        player.save_action({\"error\": \"parse_failed\"}, args.output_file)\n'''\n\nos.makedirs(\"agents\", exist_ok=True)\nwith open(\"agents/minesweeper_agent.py\", \"w\") as f:\n    f.write(AGENT_CODE)\nprint(\"Updated agents/minesweeper_agent.py\")\n\n\n# --- Write agents/minesweeper_model.py ---\nMODEL_CODE = r'''\"\"\"Minesweeper Model - Competition Version\"\"\"\nimport time\nfrom typing import Optional, Union, List\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nclass MinesweeperAgent(object):\n    def __init__(self, **kwargs):\n        model_name = \"/workspace/your_fine_tuned_model_oss\"\n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name, torch_dtype=\"auto\", device_map=\"auto\"\n        )\n\n    def generate_response(self, message, system_prompt=None, **kwargs):\n        if system_prompt is None:\n            system_prompt = 'Analyze the Minesweeper board. For each numbered cell, count adjacent flags(F) and unknowns(.). If number equals flag count, unknowns are safe to reveal. If number minus flags equals unknown count, unknowns are mines to flag. Only act on certain deductions. Output JSON with reasoning: {\"think\":\"<brief constraint reasoning>\",\"type\":\"reveal\"or\"flag\",\"row\":N,\"col\":N}'\n\n        if isinstance(message, str):\n            message = [message]\n\n        all_messages = []\n        for msg in message:\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": msg},\n            ]\n            all_messages.append(messages)\n\n        texts = []\n        for messages in all_messages:\n            try:\n                text = self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n            except TypeError:\n                text = self.tokenizer.apply_chat_template(\n                    messages, tokenize=False, add_generation_prompt=True)\n            texts.append(text)\n\n        model_inputs = self.tokenizer(\n            texts, return_tensors=\"pt\", padding=True, truncation=True\n        ).to(self.model.device)\n\n        tgps_show_var = kwargs.get(\"tgps_show\", False)\n        if tgps_show_var:\n            start_time = time.time()\n\n        generated_ids = self.model.generate(\n            **model_inputs,\n            max_new_tokens=kwargs.get(\"max_new_tokens\", 128),\n            pad_token_id=self.tokenizer.pad_token_id,\n            eos_token_id=self.tokenizer.eos_token_id,\n            temperature=kwargs.get(\"temperature\", 0.1),\n            do_sample=kwargs.get(\"do_sample\", True),\n        )\n\n        if tgps_show_var:\n            generation_time = time.time() - start_time\n\n        batch_outs = self.tokenizer.batch_decode(\n            generated_ids[:, model_inputs.input_ids.shape[1]:],\n            skip_special_tokens=True\n        )\n        batch_outs = [output.strip() for output in batch_outs]\n        print(batch_outs)\n\n        if tgps_show_var:\n            token_len = sum(len(generated_ids[i]) - model_inputs.input_ids.shape[1]\n                          for i in range(len(generated_ids)))\n            return (batch_outs[0] if len(batch_outs) == 1 else batch_outs, token_len, generation_time)\n\n        return batch_outs[0] if len(batch_outs) == 1 else batch_outs, None, None\n'''\n\nwith open(\"agents/minesweeper_model.py\", \"w\") as f:\n    f.write(MODEL_CODE)\nprint(\"Updated agents/minesweeper_model.py\")\n\n\n# --- Write minesweeper_config.yaml ---\nCONFIG_YAML = \"\"\"## Minesweeper Agent Configuration ##\nmax_new_tokens: 128\ntemperature: 0.1\ndo_sample: true\n\"\"\"\n\nwith open(\"minesweeper_config.yaml\", \"w\") as f:\n    f.write(CONFIG_YAML)\nprint(\"Updated minesweeper_config.yaml\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"ALL DONE! Inference agent files updated.\")\nprint(\"Model saved to: your_fine_tuned_model_oss/\")\nprint(\"=\" * 60)\nprint(\"\"\"\nTROUBLESHOOTING:\n- OOM: Reduce per_device_train_batch_size or num_generations\n- GRPO rewards flat: Normal for first 150 steps. If flat at 300, check reward variance.\n- Invalid JSON: Increase SFT epochs or dataset size\n- Bad on large boards: Add more large-board examples to training data\n\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Evaluation - Per-Move Breakdown\n\nTrack all 12 scoring criteria per move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-move scoring breakdown showing exactly how the model earns/loses\n# points across all 12 competition criteria. Essential for diagnosing\n# weaknesses before Phase 2 training.\n# ################################################################\n\ndef detailed_game_eval(model, tokenizer, rows, cols, num_mines, seed, max_moves=None, verbose=True):\n    \"\"\"Play a full game with detailed per-move scoring breakdown.\n    Tracks all 12 competition scoring criteria individually.\"\"\"\n    if max_moves is None:\n        max_moves = 2 * rows * cols\n\n    categories = {\n        \"safe_logical\": 0.0,\n        \"safe_random\": 0.0,\n        \"mine_hit\": 0.0,\n        \"correct_flag\": 0.0,\n        \"wrong_flag\": 0.0,\n        \"invalid_json\": 0.0,\n        \"oob\": 0.0,\n        \"already_revealed\": 0.0,\n        \"already_flagged\": 0.0,\n        \"reveal_flagged\": 0.0,\n        \"flag_revealed\": 0.0,\n        \"excess_flags\": 0.0,\n        \"win\": 0.0,\n    }\n\n    game = MinesweeperGame(rows=rows, cols=cols, num_mines=num_mines, seed=seed)\n    game.do_action({\"type\": \"reveal\", \"row\": rows // 2, \"col\": cols // 2})\n\n    total_score = 0.0\n    moves = 0\n    consecutive_bad = 0\n    result = \"ongoing\"\n\n    while game.state() == \"ongoing\" and moves < max_moves and consecutive_bad < 5:\n        prompt = build_compact_prompt(game)\n        msgs = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n                {\"role\": \"user\", \"content\": prompt}]\n        try:\n            text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                add_generation_prompt=True, enable_thinking=False)\n        except TypeError:\n            text = tokenizer.apply_chat_template(msgs, tokenize=False,\n                add_generation_prompt=True)\n        inp = tokenizer(text, return_tensors=\"pt\").to(model.device)\n        out = model.generate(**inp, temperature=0.1, max_new_tokens=128, do_sample=True)\n        resp = tokenizer.decode(out[0][inp.input_ids.shape[1]:], skip_special_tokens=True)\n        action = parse_llm_action(resp)\n        moves += 1\n        delta = 0.0\n        category = \"\"\n\n        if action is None:\n            delta = -10.0\n            category = \"invalid_json\"\n            categories[\"invalid_json\"] += delta\n            consecutive_bad += 1\n            if verbose:\n                print(f\"  Move {moves}: INVALID JSON ({resp[:60]}) -> {delta:+.0f}\")\n            total_score += delta\n            continue\n\n        try:\n            row, col = int(action[\"row\"]), int(action[\"col\"])\n        except (ValueError, TypeError):\n            delta = -10.0\n            category = \"invalid_json\"\n            categories[\"invalid_json\"] += delta\n            consecutive_bad += 1\n            if verbose:\n                print(f\"  Move {moves}: BAD ROW/COL ({action}) -> {delta:+.0f}\")\n            total_score += delta\n            continue\n\n        atype = action[\"type\"]\n\n        # Criterion 7: Out of bounds\n        if not (0 <= row < rows and 0 <= col < cols):\n            delta = -15.0\n            category = \"oob\"\n            categories[\"oob\"] += delta\n            consecutive_bad += 1\n            if verbose:\n                print(f\"  Move {moves}: OOB ({atype} {row},{col}) -> {delta:+.0f}\")\n            total_score += delta\n            continue\n\n        if atype == \"reveal\":\n            # Criterion 6: Already revealed\n            if (row, col) in game._revealed:\n                delta = -12.0\n                category = \"already_revealed\"\n                categories[\"already_revealed\"] += delta\n                consecutive_bad += 1\n                if verbose:\n                    print(f\"  Move {moves}: ALREADY REVEALED ({row},{col}) -> {delta:+.0f}\")\n                total_score += delta\n                continue\n            # Criterion 11: Reveal flagged cell\n            if (row, col) in game._flagged:\n                delta = -8.0\n                category = \"reveal_flagged\"\n                categories[\"reveal_flagged\"] += delta\n                consecutive_bad += 1\n                if verbose:\n                    print(f\"  Move {moves}: REVEAL FLAGGED ({row},{col}) -> {delta:+.0f}\")\n                total_score += delta\n                continue\n            # Criterion 3: Reveal mine\n            if game._board[row][col] == -1:\n                delta = -25.0\n                category = \"mine_hit\"\n                categories[\"mine_hit\"] += delta\n                consecutive_bad = 0\n                if verbose:\n                    print(f\"  Move {moves}: MINE HIT ({row},{col}) -> {delta:+.0f} *** GAME OVER ***\")\n                total_score += delta\n                result = \"mine_hit\"\n                break\n            # Criterion 4: Reveal safe\n            consecutive_bad = 0\n            board = game.get_visible_board()\n            is_log = is_logically_deducible(board, rows, cols, \"reveal\", row, col)\n            if is_log:\n                delta = 15.0\n                category = \"safe_logical\"\n                categories[\"safe_logical\"] += delta\n            else:\n                delta = 10.0\n                category = \"safe_random\"\n                categories[\"safe_random\"] += delta\n            game.do_action(action)\n            # Criterion 10: Win bonus\n            if game.state() == \"success\":\n                win_bonus = 100.0\n                categories[\"win\"] += win_bonus\n                delta += win_bonus\n                result = \"success\"\n                if verbose:\n                    print(f\"  Move {moves}: {category.upper()} ({row},{col}) -> +{delta:.0f} *** WIN! ***\")\n                total_score += delta\n                break\n            if verbose:\n                print(f\"  Move {moves}: {category.upper()} ({row},{col}) -> {delta:+.0f}\")\n\n        elif atype == \"flag\":\n            # Criterion 12: Flag revealed cell\n            if (row, col) in game._revealed:\n                delta = -8.0\n                category = \"flag_revealed\"\n                categories[\"flag_revealed\"] += delta\n                consecutive_bad += 1\n                if verbose:\n                    print(f\"  Move {moves}: FLAG REVEALED ({row},{col}) -> {delta:+.0f}\")\n                total_score += delta\n                continue\n            # Criterion 5: Flag already flagged\n            if (row, col) in game._flagged:\n                delta = -8.0\n                category = \"already_flagged\"\n                categories[\"already_flagged\"] += delta\n                consecutive_bad += 1\n                if verbose:\n                    print(f\"  Move {moves}: ALREADY FLAGGED ({row},{col}) -> {delta:+.0f}\")\n                total_score += delta\n                continue\n            consecutive_bad = 0\n            # Criterion 8: Excess flags\n            if len(game._flagged) + 1 > num_mines:\n                excess_pen = -10.0\n                categories[\"excess_flags\"] += excess_pen\n                delta += excess_pen\n                if verbose:\n                    print(f\"  Move {moves}: EXCESS FLAG penalty -> {excess_pen:+.0f}\")\n            # Criterion 1/2: Flag mine/non-mine\n            if game._board[row][col] == -1:\n                flag_delta = 15.0\n                category = \"correct_flag\"\n                categories[\"correct_flag\"] += flag_delta\n            else:\n                flag_delta = -10.0\n                category = \"wrong_flag\"\n                categories[\"wrong_flag\"] += flag_delta\n            delta += flag_delta\n            game.do_action(action)\n            if verbose:\n                print(f\"  Move {moves}: {category.upper()} ({row},{col}) -> {delta:+.0f}\")\n\n        total_score += delta\n\n    if result == \"ongoing\":\n        if game.state() == \"success\":\n            result = \"success\"\n        elif consecutive_bad >= 5:\n            result = \"stopped_bad_moves\"\n        elif moves >= max_moves:\n            result = \"max_moves\"\n        else:\n            result = game.state()\n\n    if verbose:\n        print(f\"\\n  --- Summary (seed={seed}) ---\")\n        print(f\"  Result: {result} | Moves: {moves} | Total Score: {total_score:+.1f}\")\n        print(f\"  {'Category':<20} {'Score':>8}\")\n        print(f\"  {'-'*28}\")\n        for cat, val in categories.items():\n            if val != 0.0:\n                print(f\"  {cat:<20} {val:>+8.1f}\")\n\n    return {\n        \"total_score\": total_score,\n        \"categories\": dict(categories),\n        \"result\": result,\n        \"moves\": moves,\n        \"seed\": seed,\n    }\n\n\nFastLanguageModel.for_inference(model)\nprint(\"=\" * 70)\nprint(\"DETAILED EVALUATION - Per-move scoring breakdown\")\nprint(\"=\" * 70)\nall_results = []\nfor bl, r, c, mi, ng in [(\"8x8\", 8, 8, 10, 2), (\"10x10\", 10, 10, 15, 2)]:\n    print(f\"\\n{'='*50} {bl} {'='*50}\")\n    for i in range(ng):\n        print(f\"\\nGame {i+1} (seed={42+i}):\")\n        res = detailed_game_eval(model, tokenizer, r, c, mi, 42 + i)\n        all_results.append((bl, res))\nprint(\"\\n\" + \"=\" * 70)\nprint(\"OVERALL SUMMARY\")\nfor bl in [\"8x8\", \"10x10\"]:\n    rs = [r for b, r in all_results if b == bl]\n    print(f\"  {bl}: {sum(1 for r in rs if r['result']=='success')}/{len(rs)} wins, \"\n          f\"avg {sum(r['total_score'] for r in rs)/len(rs):+.1f}, \"\n          f\"avg {sum(r['moves'] for r in rs)/len(rs):.1f} moves\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n\n- Model saved to `your_fine_tuned_model_oss/`\n- Agent files point to `_oss` model\n- Can run simultaneously with Qwen pipeline on separate GPU allocation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}